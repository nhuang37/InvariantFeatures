{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhuang37/ScalarModel/blob/main/Pointcloud_lightweight_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "24V0gbaJN9GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "08gINYIyLDiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "652347c9-e9ca-4535-bbd2-5af63c75add9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd '/content/drive/MyDrive/JHU/Group'"
      ],
      "metadata": {
        "id": "e7IkgpUyLI8l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUw8G1yLxYA6",
        "outputId": "8cbe7f8e-3564-4262-9cde-d65ba77f5203"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Invariant Regression) Benchmark dataset: [QM7b](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.QM7b.html)\n",
        "- Source: https://arxiv.org/pdf/1703.00564.pdf (Table 10)\n",
        "- A collection of 7,211 molecular graphs (i.e., represented as fully connected graphs describing the [Coulomb Matrix](https://singroup.github.io/dscribe/0.3.x/tutorials/coulomb_matrix.html#invariance), a $n \\times n$ symmetric matrix, invariant to translation/rotation, and requires models to be permutation invariant.\n",
        "- Task: predicting electronic\n",
        "properties of these molecule graphs\n",
        "- Note: molecules are graphs with varying sizes (deepset needed, see below)\n",
        "\n"
      ],
      "metadata": {
        "id": "5R-Yq6xXwHRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp \n",
        "import numpy as np\n",
        "import torch \n",
        "from torch_geometric.data import Data, InMemoryDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import TUDataset, zinc, QM7b\n",
        "from torch_geometric.utils import to_networkx, homophily, erdos_renyi_graph, stochastic_blockmodel_graph, scatter, loop\n",
        "from torch_geometric.nn import GINConv, global_mean_pool, global_add_pool\n",
        "from torch_geometric.nn.aggr import DeepSetsAggregation #key to implement DeepSet input (batch_features, batch_index)\n",
        "from torch_geometric.nn.models import MLP\n",
        "import torch.nn as nn\n",
        "from torch.nn import Linear, Sequential, ReLU, Dropout\n",
        "import torch_geometric.transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "bLpdxBHuxE53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 1: pre-compute fs in dataset\n",
        "#step 2: modify collate_fn in dataloader to store the edge batch index\n",
        "\n",
        "def get_fs(data):\n",
        "  '''\n",
        "  input: Data (graph) from pytorch geometric dataset\n",
        "    edge_index=[num_edges]\n",
        "    edge_attr=[num_edges]\n",
        "    y=[num_targets]\n",
        "    x=[num_nodes, 1]\n",
        "\n",
        "  output: updated DataBatch with the following additional invariant features\n",
        "   f_d is the set of diagonal edge attributes, \n",
        "   f_o is the set of off-diagonal (upper) edge attributes, \n",
        "   f_star is \\sum_{i \\neq j} X_ii X_ij where X is the n by n edge attribute graph\n",
        "  '''\n",
        "  #get f_d, f_o\n",
        "  loop_mask = data.edge_index[0] == data.edge_index[1]\n",
        "  data.f_d = data.edge_attr[loop_mask].unsqueeze(1)\n",
        "  data.f_o = data.edge_attr[~loop_mask].unsqueeze(1)\n",
        "\n",
        "  #get f_star\n",
        "  sum_fo = scatter(data.edge_attr, data.edge_index[1], dim=0) #sum over rows \n",
        "  data.f_star = (data.f_d.squeeze(1) @ ( sum_fo - data.f_d.squeeze(1))).reshape((1,1))\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "1XYRugSLg3JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The kernel trick that projects a scalar to higher-dimensional space\n",
        "#src: https://arxiv.org/pdf/1305.7074.pdf, Appendix B\n",
        "def binary_expansion(f, num_radial, theta=1):\n",
        "  '''\n",
        "  input: f (bs x 1); num_radial - number of basis expansion\n",
        "  output: phi(f) (bs x num_radial)\n",
        "  phi(f) = [..., sigmoid(f-theta/theta), sigmoid(f/theta), sigmoid(f+theta/thera),...]\n",
        "  '''\n",
        "  bs = f.shape[0]\n",
        "  out = torch.zeros((bs, num_radial))\n",
        "  max_val = (num_radial - 1)//2\n",
        "  offsets = np.arange(start=-max_val, stop=max_val+1)\n",
        "  for i, offset in enumerate(offsets):\n",
        "    out[:, i:(i+1)] = F.sigmoid( (f - theta*offset) / theta )\n",
        "  return out\n",
        "\n",
        "def get_binary_expansion(data, num_radial=1000, theta=1):\n",
        "  '''\n",
        "  Apply binary_expansion for all fs\n",
        "  try: num_radial \\in {100, 1000}\n",
        "  '''\n",
        "  #print(f\"num_basis={num_radial}\")\n",
        "  data.f_d = binary_expansion(data.f_d, num_radial, theta)\n",
        "  data.f_o = binary_expansion(data.f_o, num_radial, theta)\n",
        "  data.f_star = binary_expansion(data.f_star, num_radial, theta)\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "gOxJNC2WBtWT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "target=0\n",
        "\n",
        "class Univariate(object):\n",
        "    def __call__(self, data):\n",
        "        # Specify target.\n",
        "        data.y = data.y[:, target:(target+1)]\n",
        "        return data\n",
        "\n",
        "#path = \"/content/drive/MyDrive/JHU/Group/dataset\"\n",
        "path = \"/content/drive/MyDrive/JHU/Group/dataset_basis\"\n",
        "\n",
        "\n",
        "dataset = QM7b(path, pre_transform=T.Compose([get_fs, get_binary_expansion]), \n",
        "               transform=T.Compose([Univariate()]))\n",
        "dataset.name = \"QM7b\"\n"
      ],
      "metadata": {
        "id": "GWeom70niCpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1cb32cd-5b4b-4b02-c547-b3574a6df8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 96.9 ms, sys: 333 ms, total: 430 ms\n",
            "Wall time: 3.33 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#try out edge batch\n",
        "loader = DataLoader(dataset, batch_size=2, follow_batch=['f_d', 'f_o']) #add the keys such that we need to create batch index pointers"
      ],
      "metadata": {
        "id": "HfDkhJyAu8h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in loader:\n",
        "  print(data)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEEAXlmlvn8Q",
        "outputId": "2991d8f2-4231-4172-bbec-ff9ae0928145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataBatch(edge_index=[2, 89], edge_attr=[89], y=[2, 1], f_d=[13, 100], f_d_batch=[13], f_d_ptr=[3], f_o=[76, 100], f_o_batch=[76], f_o_ptr=[3], f_star=[2, 100], num_nodes=13, batch=[13], ptr=[3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check QM7b graph size profile\n",
        "sizes = []\n",
        "for data in dataset:\n",
        "  sizes.append(data.num_nodes)\n"
      ],
      "metadata": {
        "id": "UJ8NHd_H1ZTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(sizes)\n",
        "plt.xlabel(\"size of molecules\")\n",
        "plt.title(\"QM7b graph size distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "3fhfrgFmkf6n",
        "outputId": "b9c0e583-7e58-43e1-a9c4-6faa6bed145b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAHHCAYAAAChjmJTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHeElEQVR4nO3deVxV1eL///cBPCDKoMggV0S0ckqtuEXc1DQJHDIts5xScyzRTKtr3krRLLzatcG8Wt9Pana1bNC8OSXOqeQYzvFVc6ivgKbBcVZg/f7ox/l4ZFAMxI2v5+OxHw/2WmuvvRb7HHm7h3NsxhgjAAAAC3Er6wEAAAAUFwEGAABYDgEGAABYDgEGAABYDgEGAABYDgEGAABYDgEGAABYDgEGAABYDgEGAABYDgEGKAO9e/dW5cqVy3oYJa5WrVp65JFHSq3/1atXy2azafXq1aW2j2uVkJAgm83mUlarVi317t271Pd96NAh2Ww2zZw501l2o19TNptNCQkJN2x/wJUIMLgl7N69Wz169NBf/vIXeXp6KjQ0VD169NCePXvytZ05c6ZsNptsNpvWrVuXr94Yo7CwMNlsNpc/1nl/XAtb3nzzzVKdI6xp8eLFN20QuJnHBniU9QCA0jZv3jx17dpVVatWVd++fRUREaFDhw7p448/1ldffaW5c+eqQ4cO+bbz8vLSnDlz1LRpU5fyNWvW6Ndff5Wnp6dLef369fXpp5/m6+fTTz/VsmXLFBsbW7ITuwU1b95c586dk91uL+uhFCg1NVVubsX7f+HixYs1ZcqUYgWF8PBwnTt3ThUqVCjmCIunqLGdO3dOHh78CUHZ4dWHcu3AgQN6+umnVbt2ba1du1aBgYHOuqFDh6pZs2bq0aOHduzYoYiICJdt27Ztqy+//FLvv/++yz/Uc+bMUWRkpH777TeX9sHBwerRo0e+MYwZM0a333677r333hKe3fU5c+aMKlWqVNbDuC5ubm7y8vIq62EU6spQW9Kys7OVm5sru91e5r+Hst4/wCUklGsTJ07U2bNn9dFHH7mEF0mqVq2aPvzwQ50+fVoTJ07Mt23Xrl114sQJJSUlOcsuXryor776St26dbum/W/atEn79+9X9+7dC6z/+eefFRcXp0qVKik0NFRjx47VtXxBfG5urhISEhQaGipvb2+1bNlSe/bsyXcPRt7lsDVr1mjQoEEKCgpSjRo1JEmHDx/WoEGDVLduXVWsWFEBAQHq3LmzDh065LKvvD7Wrl2rgQMHKiAgQL6+vurZs6d+//33Ase3bt063XffffLy8lLt2rU1a9asa/p9ff7554qMjJSPj498fX3VqFEjvffee876K++Bufxy35VLixYtXPr+z3/+o8jISFWsWFFVq1ZVly5d9Msvv1zTuNatW6d7771XXl5eqlOnjj788MMC2135+7906ZIzwHp5eSkgIEBNmzZ1vqZ69+6tKVOmSJLL2KX/vc/l7bff1rvvvqs6derI09NTe/bsKfAemDxXe00Vdh/RlX0WNba8sivPzPz4449q06aNfH19VblyZbVq1Uo//PCDS5u8Y7Z+/XoNHz5cgYGBqlSpkh577DEdP3684AMAFIAzMCjXvv32W9WqVUvNmjUrsL558+aqVauWvv32W/373/92qatVq5aio6P12WefqU2bNpKkJUuWKCsrS126dNH7779/1f3Pnj1bkgoMMDk5OWrdurXuv/9+TZgwQUuXLtXo0aOVnZ2tsWPHFtnvyJEjNWHCBLVv315xcXHavn274uLidP78+QLbDxo0SIGBgRo1apTOnDkjSdq8ebM2bNigLl26qEaNGjp06JCmTp2qFi1aaM+ePfL29nbpY/DgwfL391dCQoJSU1M1depUHT582PkHMc/+/fv1xBNPqG/fvurVq5emT5+u3r17KzIyUg0bNix0TklJSeratatatWqlf/7zn5KkvXv3av369Ro6dGiB2zRv3jzfZbvDhw/rtddeU1BQkLPszTff1Ouvv64nn3xS/fr10/HjxzV58mQ1b95cP/74o/z9/Qsd186dOxUbG6vAwEAlJCQoOztbo0ePVnBwcKHb5ElISFBiYqL69eun++67Tw6HQ1u2bNG2bdv08MMPa+DAgTp69KiSkpIKvPwoSTNmzND58+c1YMAAeXp6qmrVqsrNzS2w7Z95TV3pWsZ2ud27d6tZs2by9fXV3//+d1WoUEEffvihWrRooTVr1igqKsql/ZAhQ1SlShWNHj1ahw4d0rvvvqvBgwdr7ty5xRonbmEGKKcyMzONJNOhQ4ci2z366KNGknE4HMYYY2bMmGEkmc2bN5sPPvjA+Pj4mLNnzxpjjOncubNp2bKlMcaY8PBw065du0L7zc7ONsHBwea+++7LV9erVy8jyQwZMsRZlpuba9q1a2fsdrs5fvx4of2mp6cbDw8P07FjR5fyhIQEI8n06tXLWZY3l6ZNm5rs7GyX9nlzulxycrKRZGbNmpWvj8jISHPx4kVn+YQJE4wks2DBAmdZeHi4kWTWrl3rLDt27Jjx9PQ0L774YqFzMsaYoUOHGl9f33zjvNyqVauMJLNq1aoC68+dO2ciIyNNaGioSUtLM8YYc+jQIePu7m7efPNNl7Y7d+40Hh4e+cqv1LFjR+Pl5WUOHz7sLNuzZ49xd3c3V/4TGh4e7vL7b9KkSZGvEWOMiY+Pz9ePMcYcPHjQSDK+vr7m2LFjBdbNmDHDWXatr6nCfocF9VnY2IwxRpIZPXq0c71jx47GbrebAwcOOMuOHj1qfHx8TPPmzZ1lea+nmJgYk5ub6ywfNmyYcXd3N5mZmQXuD7gSl5BQbp06dUqS5OPjU2S7vPq89pd78sknde7cOS1cuFCnTp3SwoULr/ny0YoVK5SRkVHo5SPpj7MaeWw2mwYPHqyLFy9q+fLlRfabnZ2tQYMGuZQPGTKk0G369+8vd3d3l7KKFSs6f7506ZJOnDih2267Tf7+/tq2bVu+PgYMGOBy0+hzzz0nDw8PLV682KVdgwYNXM54BQYGqm7duvr5558LHZ8k+fv768yZMy6X7Ipr0KBB2rlzp77++muFhIRI+uMm7tzcXD355JP67bffnEtISIhuv/12rVq1qtD+cnJy9N1336ljx46qWbOms7x+/fqKi4u76nj8/f21e/du7du377rn1KlTp3yXP4tyPa+pPysnJ0fLli1Tx44dVbt2bWd59erV1a1bN61bt04Oh8NlmwEDBricuWvWrJlycnJ0+PDhUhsnyhcCDMqtooLJ5U6dOiWbzaZq1arlqwsMDFRMTIzmzJmjefPmKScnR0888cQ17X/27Nlyd3fXU089VWC9m5ubyz/2knTHHXdIUr77UC6X9w/8bbfd5lJetWpVValSpcBtrrxBWfrjKZJRo0YpLCxMnp6eqlatmgIDA5WZmamsrKx87W+//XaX9cqVK6t69er5xnr5H/o8VapUKfR+mTyDBg3SHXfcoTZt2qhGjRrq06ePli5dWuQ2l/vwww81Y8YMTZ48Wffff7+zfN++fTLG6Pbbb1dgYKDLsnfvXh07dqzQPo8fP65z587lm7sk1a1b96pjGjt2rDIzM3XHHXeoUaNGevnll7Vjx45rnpNU8LErzPW+pv6s48eP6+zZswX+TurXr6/c3Nx89xtd+TrJe+1e7XUC5OEeGJRbfn5+Cg0NveofjB07dqhGjRqFPprbrVs39e/fX+np6WrTpk2R90vkOXfunObPn6+YmJhruleitF1+tiXPkCFDNGPGDL3wwguKjo6Wn5+fbDabunTpUug9FtfiyjM9ecxVbk4OCgpSSkqKvvvuOy1ZskRLlizRjBkz1LNnT33yySdFbrtp0yYNHTpU/fr104ABA1zqcnNzZbPZtGTJkgLHVpof/ta8eXMdOHBACxYs0LJly/Q///M/eueddzRt2jT169fvmvoo6Nj9GVd++F6enJycEt3P1Vzv6wTIwxkYlGvt27fXwYMHC/xAOkn6/vvvdejQIXXu3LnQPh577DG5ubnphx9+uObLR//973916tSpIi8f5ebm5rus8n//7/+V9McNxIUJDw+X9MfNspc7ceJEsf73+tVXX6lXr17617/+pSeeeEIPP/ywmjZtqszMzALbX3kZ5PTp00pLSytyrMVlt9vVvn17/fvf/9aBAwc0cOBAzZo1K99cL3f8+HE98cQTuuuuu5xPzVyuTp06MsYoIiJCMTEx+ZbLz9ZcKTAwUBUrVizwElBqauo1zalq1ap65pln9Nlnn+mXX35R48aNXZ7eKSxQXI9reU3lnem48jgXdOnmWscWGBgob2/vAn8nP/30k9zc3BQWFnZNfQHXigCDcu2ll16St7e3Bg4cqBMnTrjUnTx5Us8++6x8fX1d7hu4UuXKlTV16lQlJCSoffv217TfOXPmyNvbW4899liR7T744APnz8YYffDBB6pQoYJatWpV6DatWrWSh4eHpk6dWmhf18Ld3T3f/3YnT55c6P/EP/roI126dMm5PnXqVGVnZzuf0Pqzrjw+bm5uaty4sSTpwoULBW6Tk5OjLl266OLFi/r6668LPIv2+OOPy93dXWPGjMk3X2NMvv1ezt3dXXFxcfrmm2905MgRZ/nevXv13XffFXtOlStX1m233eYyn7zP5CksOBbX1V5T4eHhcnd319q1a122u/IpvOKMzd3dXbGxsVqwYIHLpaqMjAznh0H6+vpe54yAgnEJCeXabbfdplmzZqlr165q1KhRvk/i/f333/X5559f9T6DXr16XfM+T548qSVLlqhTp05FXp7w8vLS0qVL1atXL0VFRWnJkiVatGiR/vGPfxR502ZwcLCGDh2qf/3rX3r00UfVunVrbd++XUuWLFG1atWu+X/NjzzyiD799FP5+fmpQYMGSk5O1vLlyxUQEFBg+4sXL6pVq1Z68sknlZqaqn//+99q2rSpHn300Wva39X069dPJ0+e1EMPPaQaNWro8OHDmjx5su666y7Vr1+/wG2mTZumlStX6tlnn813M25wcLAefvhh1alTR+PGjdPIkSN16NAhdezYUT4+Pjp48KDmz5+vAQMG6KWXXip0XGPGjNHSpUvVrFkzDRo0SNnZ2Zo8ebIaNmx41cuTDRo0UIsWLRQZGamqVatqy5Yt+uqrr1wCc2RkpCTp+eefV1xcnNzd3dWlS5dr/bW5uJbXlJ+fnzp37qzJkyfLZrOpTp06WrhwYYH3AhVnbOPGjVNSUpKaNm2qQYMGycPDQx9++KEuXLigCRMmXNd8gCKV2fNPwA20c+dO061bNxMSEmLc3NyMJOPl5WV2796dr+3lj1EXpbDHqKdNm2Ykmf/+97+FbturVy9TqVIlc+DAARMbG2u8vb1NcHCwGT16tMnJybnqfLKzs83rr79uQkJCTMWKFc1DDz1k9u7dawICAsyzzz57TXP5/fffzTPPPGOqVatmKleubOLi4sxPP/2U71HgvD7WrFljBgwYYKpUqWIqV65sunfvbk6cOHFNv5MHH3zQPPjgg0XO6auvvjKxsbEmKCjI2O12U7NmTTNw4EDn49DG5H8EePTo0UZSgcuV+/v6669N06ZNTaVKlUylSpVMvXr1THx8vElNTS1yXMYYs2bNGhMZGWnsdrupXbu2mTZtmnPfV87/8t/duHHjzH333Wf8/f1NxYoVTb169cybb77p8jh6dna2GTJkiAkMDDQ2m83ZZ95jzRMnTsw3nsIeo77W19Tx48dNp06djLe3t6lSpYoZOHCg2bVrV74+CxubMfkfozbGmG3btpm4uDhTuXJl4+3tbVq2bGk2bNjg0qaw1+TVHpEHrmQzhjumcOuZNWuWevfurR49elzzp8Te7DIzM1WlShWNGzdOr776aon1O3PmTD3zzDPavHmz/vrXv5ZYvwDwZ3AJCbeknj17Ki0tTa+88opq1Kiht956q6yHVCznzp3L93TKu+++K0n5PkIfAMojAgxuWSNGjNCIESPKehjXZe7cuZo5c6batm2rypUra926dfrss88UGxurBx54oKyHBwCljgADWFDjxo3l4eGhCRMmyOFwOG/sHTduXFkPDQBuCO6BAQAAlsPnwAAAAMshwAAAAMspt/fA5Obm6ujRo/Lx8SnRj+oGAAClxxijU6dOKTQ0VG5uhZ9nKbcB5ujRo3z3BgAAFvXLL7+oRo0ahdaX2wDj4+Mj6Y9fAN/BAQCANTgcDoWFhTn/jhem3AaYvMtGvr6+BBgAACzmard/cBMvAACwHAIMAACwHAIMAACwHAIMAACwHAIMAACwHAIMAACwHAIMAACwHAIMAACwnGIFmMTERN17773y8fFRUFCQOnbsqNTUVJc258+fV3x8vAICAlS5cmV16tRJGRkZLm2OHDmidu3aydvbW0FBQXr55ZeVnZ3t0mb16tW655575Onpqdtuu00zZ868vhkCAIByp1gBZs2aNYqPj9cPP/ygpKQkXbp0SbGxsTpz5oyzzbBhw/Ttt9/qyy+/1Jo1a3T06FE9/vjjzvqcnBy1a9dOFy9e1IYNG/TJJ59o5syZGjVqlLPNwYMH1a5dO7Vs2VIpKSl64YUX1K9fP3333XclMGUAAGB1NmOMud6Njx8/rqCgIK1Zs0bNmzdXVlaWAgMDNWfOHD3xxBOSpJ9++kn169dXcnKy7r//fi1ZskSPPPKIjh49quDgYEnStGnTNGLECB0/flx2u10jRozQokWLtGvXLue+unTposzMTC1duvSaxuZwOOTn56esrCy+SgAAAIu41r/ff+oemKysLElS1apVJUlbt27VpUuXFBMT42xTr1491axZU8nJyZKk5ORkNWrUyBleJCkuLk4Oh0O7d+92trm8j7w2eX0U5MKFC3I4HC4LAAAon647wOTm5uqFF17QAw88oDvvvFOSlJ6eLrvdLn9/f5e2wcHBSk9Pd7a5PLzk1efVFdXG4XDo3LlzBY4nMTFRfn5+ziUsLOx6pwYAAG5y1x1g4uPjtWvXLn3++eclOZ7rNnLkSGVlZTmXX375payHBAAASonH9Ww0ePBgLVy4UGvXrlWNGjWc5SEhIbp48aIyMzNdzsJkZGQoJCTE2WbTpk0u/eU9pXR5myufXMrIyJCvr68qVqxY4Jg8PT3l6el5PdMBAAAWU6wAY4zRkCFDNH/+fK1evVoREREu9ZGRkapQoYJWrFihTp06SZJSU1N15MgRRUdHS5Kio6P15ptv6tixYwoKCpIkJSUlydfXVw0aNHC2Wbx4sUvfSUlJzj4AoNYri8p6CMV2aHy7sh4CUG4UK8DEx8drzpw5WrBggXx8fJz3rPj5+alixYry8/NT3759NXz4cFWtWlW+vr4aMmSIoqOjdf/990uSYmNj1aBBAz399NOaMGGC0tPT9dprryk+Pt55BuXZZ5/VBx98oL///e/q06ePVq5cqS+++EKLFlnvHywAAFDyinUPzNSpU5WVlaUWLVqoevXqzmXu3LnONu+8844eeeQRderUSc2bN1dISIjmzZvnrHd3d9fChQvl7u6u6Oho9ejRQz179tTYsWOdbSIiIrRo0SIlJSWpSZMm+te//qX/+Z//UVxcXAlMGQAAWN2f+hyYmxmfAwOUb1xCAsqnG/I5MAAAAGWBAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACzHo6wHAAC3ilqvLCrrIRTbofHtynoIQIE4AwMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyHAAMAACyn2AFm7dq1at++vUJDQ2Wz2fTNN9+41NtstgKXiRMnOtvUqlUrX/348eNd+tmxY4eaNWsmLy8vhYWFacKECdc3QwAAUO4UO8CcOXNGTZo00ZQpUwqsT0tLc1mmT58um82mTp06ubQbO3asS7shQ4Y46xwOh2JjYxUeHq6tW7dq4sSJSkhI0EcffVTc4QIAgHLIo7gbtGnTRm3atCm0PiQkxGV9wYIFatmypWrXru1S7uPjk69tntmzZ+vixYuaPn267Ha7GjZsqJSUFE2aNEkDBgwo7pABAEA5U6r3wGRkZGjRokXq27dvvrrx48crICBAd999tyZOnKjs7GxnXXJyspo3by673e4si4uLU2pqqn7//ffSHDIAALCAYp+BKY5PPvlEPj4+evzxx13Kn3/+ed1zzz2qWrWqNmzYoJEjRyotLU2TJk2SJKWnpysiIsJlm+DgYGddlSpV8u3rwoULunDhgnPd4XCU9HQAAMBNolQDzPTp09W9e3d5eXm5lA8fPtz5c+PGjWW32zVw4EAlJibK09PzuvaVmJioMWPG/KnxAgAAayi1S0jff/+9UlNT1a9fv6u2jYqKUnZ2tg4dOiTpj/toMjIyXNrkrRd238zIkSOVlZXlXH755Zc/NwEAAHDTKrUA8/HHHysyMlJNmjS5atuUlBS5ubkpKChIkhQdHa21a9fq0qVLzjZJSUmqW7dugZePJMnT01O+vr4uCwAAKJ+KHWBOnz6tlJQUpaSkSJIOHjyolJQUHTlyxNnG4XDoyy+/LPDsS3Jyst59911t375dP//8s2bPnq1hw4apR48eznDSrVs32e129e3bV7t379bcuXP13nvvuVx6AgAAt65i3wOzZcsWtWzZ0rmeFyp69eqlmTNnSpI+//xzGWPUtWvXfNt7enrq888/V0JCgi5cuKCIiAgNGzbMJZz4+flp2bJlio+PV2RkpKpVq6ZRo0bxCDUAAJAk2YwxpqwHURocDof8/PyUlZXF5SSgHKr1yqKyHsIt4dD4dmU9BNxirvXvN9+FBAAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALKfYAWbt2rVq3769QkNDZbPZ9M0337jU9+7dWzabzWVp3bq1S5uTJ0+qe/fu8vX1lb+/v/r27avTp0+7tNmxY4eaNWsmLy8vhYWFacKECcWfHQAAKJeKHWDOnDmjJk2aaMqUKYW2ad26tdLS0pzLZ5995lLfvXt37d69W0lJSVq4cKHWrl2rAQMGOOsdDodiY2MVHh6urVu3auLEiUpISNBHH31U3OECAIByyKO4G7Rp00Zt2rQpso2np6dCQkIKrNu7d6+WLl2qzZs3669//askafLkyWrbtq3efvtthYaGavbs2bp48aKmT58uu92uhg0bKiUlRZMmTXIJOgAA4NZUKvfArF69WkFBQapbt66ee+45nThxwlmXnJwsf39/Z3iRpJiYGLm5uWnjxo3ONs2bN5fdbne2iYuLU2pqqn7//fcC93nhwgU5HA6XBQAAlE8lHmBat26tWbNmacWKFfrnP/+pNWvWqE2bNsrJyZEkpaenKygoyGUbDw8PVa1aVenp6c42wcHBLm3y1vPaXCkxMVF+fn7OJSwsrKSnBgAAbhLFvoR0NV26dHH+3KhRIzVu3Fh16tTR6tWr1apVq5LendPIkSM1fPhw57rD4SDEAABQTpX6Y9S1a9dWtWrVtH//fklSSEiIjh075tImOztbJ0+edN43ExISooyMDJc2eeuF3Vvj6ekpX19flwUAAJRPpR5gfv31V504cULVq1eXJEVHRyszM1Nbt251tlm5cqVyc3MVFRXlbLN27VpdunTJ2SYpKUl169ZVlSpVSnvIAADgJlfsAHP69GmlpKQoJSVFknTw4EGlpKToyJEjOn36tF5++WX98MMPOnTokFasWKEOHTrotttuU1xcnCSpfv36at26tfr3769NmzZp/fr1Gjx4sLp06aLQ0FBJUrdu3WS329W3b1/t3r1bc+fO1XvvvedyiQgAANy6ih1gtmzZorvvvlt33323JGn48OG6++67NWrUKLm7u2vHjh169NFHdccdd6hv376KjIzU999/L09PT2cfs2fPVr169dSqVSu1bdtWTZs2dfmMFz8/Py1btkwHDx5UZGSkXnzxRY0aNYpHqAEAgCTJZowxZT2I0uBwOOTn56esrCzuhwHKoVqvLCrrIdwSDo1vV9ZDwC3mWv9+811IAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcjyKu8HatWs1ceJEbd26VWlpaZo/f746duwoSbp06ZJee+01LV68WD///LP8/PwUExOj8ePHKzQ01NlHrVq1dPjwYZd+ExMT9corrzjXd+zYofj4eG3evFmBgYEaMmSI/v73v1/nNAEUpdYri8p6CABQLMU+A3PmzBk1adJEU6ZMyVd39uxZbdu2Ta+//rq2bdumefPmKTU1VY8++mi+tmPHjlVaWppzGTJkiLPO4XAoNjZW4eHh2rp1qyZOnKiEhAR99NFHxR0uAAAoh4p9BqZNmzZq06ZNgXV+fn5KSkpyKfvggw9033336ciRI6pZs6az3MfHRyEhIQX2M3v2bF28eFHTp0+X3W5Xw4YNlZKSokmTJmnAgAHFHTIAAChnSv0emKysLNlsNvn7+7uUjx8/XgEBAbr77rs1ceJEZWdnO+uSk5PVvHlz2e12Z1lcXJxSU1P1+++/F7ifCxcuyOFwuCwAAKB8KvYZmOI4f/68RowYoa5du8rX19dZ/vzzz+uee+5R1apVtWHDBo0cOVJpaWmaNGmSJCk9PV0REREufQUHBzvrqlSpkm9fiYmJGjNmTCnOBgAA3CxKLcBcunRJTz75pIwxmjp1qkvd8OHDnT83btxYdrtdAwcOVGJiojw9Pa9rfyNHjnTp1+FwKCws7PoGDwAAbmqlEmDywsvhw4e1cuVKl7MvBYmKilJ2drYOHTqkunXrKiQkRBkZGS5t8tYLu2/G09PzusMPAACwlhK/ByYvvOzbt0/Lly9XQEDAVbdJSUmRm5ubgoKCJEnR0dFau3atLl265GyTlJSkunXrFnj5CAAA3FqKfQbm9OnT2r9/v3P94MGDSklJUdWqVVW9enU98cQT2rZtmxYuXKicnBylp6dLkqpWrSq73a7k5GRt3LhRLVu2lI+Pj5KTkzVs2DD16NHDGU66deumMWPGqG/fvhoxYoR27dql9957T++8804JTRsAAFiZzRhjirPB6tWr1bJly3zlvXr1UkJCQr6bb/OsWrVKLVq00LZt2zRo0CD99NNPunDhgiIiIvT0009r+PDhLpeALv8gu2rVqmnIkCEaMWLENY/T4XDIz89PWVlZV72EBdzq+CA7FObQ+HZlPQTcYq7173exA4xVEGCAa0eAQWEIMLjRrvXvN9+FBAAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALIcAAwAALMejrAcAALh51XplUVkPodgOjW9X1kPADcAZGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDkEGAAAYDnFDjBr165V+/btFRoaKpvNpm+++cal3hijUaNGqXr16qpYsaJiYmK0b98+lzYnT55U9+7d5evrK39/f/Xt21enT592abNjxw41a9ZMXl5eCgsL04QJE4o/OwAAUC4VO8CcOXNGTZo00ZQpUwqsnzBhgt5//31NmzZNGzduVKVKlRQXF6fz588723Tv3l27d+9WUlKSFi5cqLVr12rAgAHOeofDodjYWIWHh2vr1q2aOHGiEhIS9NFHH13HFAEAQHljM8aY697YZtP8+fPVsWNHSX+cfQkNDdWLL76ol156SZKUlZWl4OBgzZw5U126dNHevXvVoEEDbd68WX/9618lSUuXLlXbtm3166+/KjQ0VFOnTtWrr76q9PR02e12SdIrr7yib775Rj/99NM1jc3hcMjPz09ZWVny9fW93ikCt4Rarywq6yEAJebQ+HZlPQT8Cdf697tE74E5ePCg0tPTFRMT4yzz8/NTVFSUkpOTJUnJycny9/d3hhdJiomJkZubmzZu3Ohs07x5c2d4kaS4uDilpqbq999/L3DfFy5ckMPhcFkAAED5VKIBJj09XZIUHBzsUh4cHOysS09PV1BQkEu9h4eHqlat6tKmoD4u38eVEhMT5efn51zCwsL+/IQAAMBNqdw8hTRy5EhlZWU5l19++aWshwQAAEpJiQaYkJAQSVJGRoZLeUZGhrMuJCREx44dc6nPzs7WyZMnXdoU1Mfl+7iSp6enfH19XRYAAFA+lWiAiYiIUEhIiFasWOEsczgc2rhxo6KjoyVJ0dHRyszM1NatW51tVq5cqdzcXEVFRTnbrF27VpcuXXK2SUpKUt26dVWlSpWSHDIAALCgYgeY06dPKyUlRSkpKZL+uHE3JSVFR44ckc1m0wsvvKBx48bpv//9r3bu3KmePXsqNDTU+aRS/fr11bp1a/Xv31+bNm3S+vXrNXjwYHXp0kWhoaGSpG7duslut6tv377avXu35s6dq/fee0/Dhw8vsYkDAADr8ijuBlu2bFHLli2d63mholevXpo5c6b+/ve/68yZMxowYIAyMzPVtGlTLV26VF5eXs5tZs+ercGDB6tVq1Zyc3NTp06d9P777zvr/fz8tGzZMsXHxysyMlLVqlXTqFGjXD4rBgAA3Lr+1OfA3Mz4HBjg2vE5MChP+BwYayuTz4EBAAC4EQgwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcggwAADAcko8wNSqVUs2my3fEh8fL0lq0aJFvrpnn33WpY8jR46oXbt28vb2VlBQkF5++WVlZ2eX9FABAIBFeZR0h5s3b1ZOTo5zfdeuXXr44YfVuXNnZ1n//v01duxY57q3t7fz55ycHLVr104hISHasGGD0tLS1LNnT1WoUEFvvfVWSQ8XAABYUIkHmMDAQJf18ePHq06dOnrwwQedZd7e3goJCSlw+2XLlmnPnj1avny5goODddddd+mNN97QiBEjlJCQILvdXtJDBgAAFlOq98BcvHhR//nPf9SnTx/ZbDZn+ezZs1WtWjXdeeedGjlypM6ePeusS05OVqNGjRQcHOwsi4uLk8Ph0O7duwvd14ULF+RwOFwWAABQPpX4GZjLffPNN8rMzFTv3r2dZd26dVN4eLhCQ0O1Y8cOjRgxQqmpqZo3b54kKT093SW8SHKup6enF7qvxMREjRkzpuQnAQAAbjqlGmA+/vhjtWnTRqGhoc6yAQMGOH9u1KiRqlevrlatWunAgQOqU6fOde9r5MiRGj58uHPd4XAoLCzsuvsDAAA3r1ILMIcPH9by5cudZ1YKExUVJUnav3+/6tSpo5CQEG3atMmlTUZGhiQVet+MJHl6esrT0/NPjhoAAFhBqd0DM2PGDAUFBaldu3ZFtktJSZEkVa9eXZIUHR2tnTt36tixY842SUlJ8vX1VYMGDUpruAAAwEJK5QxMbm6uZsyYoV69esnD4393ceDAAc2ZM0dt27ZVQECAduzYoWHDhql58+Zq3LixJCk2NlYNGjTQ008/rQkTJig9PV2vvfaa4uPjOcMCAAAklVKAWb58uY4cOaI+ffq4lNvtdi1fvlzvvvuuzpw5o7CwMHXq1Emvvfaas427u7sWLlyo5557TtHR0apUqZJ69erl8rkxAADg1lYqASY2NlbGmHzlYWFhWrNmzVW3Dw8P1+LFi0tjaAAAoBzgu5AAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDllHiASUhIkM1mc1nq1avnrD9//rzi4+MVEBCgypUrq1OnTsrIyHDp48iRI2rXrp28vb0VFBSkl19+WdnZ2SU9VAAAYFEepdFpw4YNtXz58v/dicf/7mbYsGFatGiRvvzyS/n5+Wnw4MF6/PHHtX79eklSTk6O2rVrp5CQEG3YsEFpaWnq2bOnKlSooLfeeqs0hgsAACymVAKMh4eHQkJC8pVnZWXp448/1pw5c/TQQw9JkmbMmKH69evrhx9+0P33369ly5Zpz549Wr58uYKDg3XXXXfpjTfe0IgRI5SQkCC73V4aQwYAABZSKvfA7Nu3T6Ghoapdu7a6d++uI0eOSJK2bt2qS5cuKSYmxtm2Xr16qlmzppKTkyVJycnJatSokYKDg51t4uLi5HA4tHv37kL3eeHCBTkcDpcFAACUTyUeYKKiojRz5kwtXbpUU6dO1cGDB9WsWTOdOnVK6enpstvt8vf3d9kmODhY6enpkqT09HSX8JJXn1dXmMTERPn5+TmXsLCwkp0YAAC4aZT4JaQ2bdo4f27cuLGioqIUHh6uL774QhUrVizp3TmNHDlSw4cPd647HA5CDAAA5VSpP0bt7++vO+64Q/v371dISIguXryozMxMlzYZGRnOe2ZCQkLyPZWUt17QfTV5PD095evr67IAAIDyqdQDzOnTp3XgwAFVr15dkZGRqlChglasWOGsT01N1ZEjRxQdHS1Jio6O1s6dO3Xs2DFnm6SkJPn6+qpBgwalPVwAAGABJX4J6aWXXlL79u0VHh6uo0ePavTo0XJ3d1fXrl3l5+envn37avjw4apatap8fX01ZMgQRUdH6/7775ckxcbGqkGDBnr66ac1YcIEpaen67XXXlN8fLw8PT1LergAAMCCSjzA/Prrr+ratatOnDihwMBANW3aVD/88IMCAwMlSe+8847c3NzUqVMnXbhwQXFxcfr3v//t3N7d3V0LFy7Uc889p+joaFWqVEm9evXS2LFjS3qoAADAomzGGFPWgygNDodDfn5+ysrK4n4Y4CpqvbKorIcAlJhD49uV9RDwJ1zr32++CwkAAFgOAQYAAFgOAQYAAFgOAQYAAFgOAQYAAFgOAQYAAFgOAQYAAFgOAQYAAFhOiX8SL3Cr40PhAKD0cQYGAABYDgEGAABYDgEGAABYDgEGAABYDjfxAgDKFSveSM83aBcfZ2AAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDllHiASUxM1L333isfHx8FBQWpY8eOSk1NdWnTokUL2Ww2l+XZZ591aXPkyBG1a9dO3t7eCgoK0ssvv6zs7OySHi4AALAgj5LucM2aNYqPj9e9996r7Oxs/eMf/1BsbKz27NmjSpUqOdv1799fY8eOda57e3s7f87JyVG7du0UEhKiDRs2KC0tTT179lSFChX01ltvlfSQAQCAxZR4gFm6dKnL+syZMxUUFKStW7eqefPmznJvb2+FhIQU2MeyZcu0Z88eLV++XMHBwbrrrrv0xhtvaMSIEUpISJDdbi/pYQMAAAsp9XtgsrKyJElVq1Z1KZ89e7aqVaumO++8UyNHjtTZs2eddcnJyWrUqJGCg4OdZXFxcXI4HNq9e3eB+7lw4YIcDofLAgAAyqcSPwNzudzcXL3wwgt64IEHdOeddzrLu3XrpvDwcIWGhmrHjh0aMWKEUlNTNW/ePElSenq6S3iR5FxPT08vcF+JiYkaM2ZMKc0EAADcTEo1wMTHx2vXrl1at26dS/mAAQOcPzdq1EjVq1dXq1atdODAAdWpU+e69jVy5EgNHz7cue5wOBQWFnZ9AwcAADe1UruENHjwYC1cuFCrVq1SjRo1imwbFRUlSdq/f78kKSQkRBkZGS5t8tYLu2/G09NTvr6+LgsAACifSjzAGGM0ePBgzZ8/XytXrlRERMRVt0lJSZEkVa9eXZIUHR2tnTt36tixY842SUlJ8vX1VYMGDUp6yAAAwGJK/BJSfHy85syZowULFsjHx8d5z4qfn58qVqyoAwcOaM6cOWrbtq0CAgK0Y8cODRs2TM2bN1fjxo0lSbGxsWrQoIGefvppTZgwQenp6XrttdcUHx8vT0/Pkh4yAACwmBI/AzN16lRlZWWpRYsWql69unOZO3euJMlut2v58uWKjY1VvXr19OKLL6pTp0769ttvnX24u7tr4cKFcnd3V3R0tHr06KGePXu6fG4MAAC4dZX4GRhjTJH1YWFhWrNmzVX7CQ8P1+LFi0tqWAAAoBzhu5AAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDlEGAAAIDleJT1AICi1HplUVkPAQBwE+IMDAAAsBwCDAAAsBwCDAAAsBwCDAAAsBwCDAAAsByeQgIAoIxZ8YnLQ+Pblen+OQMDAAAshwADAAAshwADAAAs56YOMFOmTFGtWrXk5eWlqKgobdq0qayHBAAAbgI3bYCZO3euhg8frtGjR2vbtm1q0qSJ4uLidOzYsbIeGgAAKGM3bYCZNGmS+vfvr2eeeUYNGjTQtGnT5O3trenTp5f10AAAQBm7KR+jvnjxorZu3aqRI0c6y9zc3BQTE6Pk5OQyHJl1WfERPQAACnNTBpjffvtNOTk5Cg4OdikPDg7WTz/9VOA2Fy5c0IULF5zrWVlZkiSHw1Hi47tz9Hcl3icAAFZSGn9fL+/XGFNku5sywFyPxMREjRkzJl95WFhYGYwGAIDyze/d0u3/1KlT8vPzK7T+pgww1apVk7u7uzIyMlzKMzIyFBISUuA2I0eO1PDhw53rubm5OnnypAICAmSz2Up1vCXJ4XAoLCxMv/zyi3x9fct6OKXqVpqrdGvNl7mWX7fSfJlr2TDG6NSpUwoNDS2y3U0ZYOx2uyIjI7VixQp17NhR0h+BZMWKFRo8eHCB23h6esrT09OlzN/fv5RHWnp8fX3L/EV0o9xKc5Vurfky1/LrVpovc73xijrzkuemDDCSNHz4cPXq1Ut//etfdd999+ndd9/VmTNn9Mwzz5T10AAAQBm7aQPMU089pePHj2vUqFFKT0/XXXfdpaVLl+a7sRcAANx6btoAI0mDBw8u9JJReeXp6anRo0fnuxxWHt1Kc5Vurfky1/LrVpovc7252czVnlMCAAC4ydy0n8QLAABQGAIMAACwHAIMAACwHAIMAACwHALMDZSQkCCbzeay1KtXr8htvvzyS9WrV09eXl5q1KiRFi9efING++fVqlUr33xtNpvi4+MLbD9z5sx8bb28vG7wqK/N2rVr1b59e4WGhspms+mbb75xqTfGaNSoUapevboqVqyomJgY7du376r9TpkyRbVq1ZKXl5eioqK0adOmUprBtStqrpcuXdKIESPUqFEjVapUSaGhoerZs6eOHj1aZJ/X8164Ua52bHv37p1v7K1bt75qv1Y7tpIKfP/abDZNnDix0D5v1mObmJioe++9Vz4+PgoKClLHjh2Vmprq0ub8+fOKj49XQECAKleurE6dOuX7RPgrXe97vTRdba4nT57UkCFDVLduXVWsWFE1a9bU888/7/wOwcJc72u/tBBgbrCGDRsqLS3Nuaxbt67Qths2bFDXrl3Vt29f/fjjj+rYsaM6duyoXbt23cARX7/Nmze7zDUpKUmS1Llz50K38fX1ddnm8OHDN2q4xXLmzBk1adJEU6ZMKbB+woQJev/99zVt2jRt3LhRlSpVUlxcnM6fP19on3PnztXw4cM1evRobdu2TU2aNFFcXJyOHTtWWtO4JkXN9ezZs9q2bZtef/11bdu2TfPmzVNqaqoeffTRq/ZbnPfCjXS1YytJrVu3dhn7Z599VmSfVjy2klzmmJaWpunTp8tms6lTp05F9nszHts1a9YoPj5eP/zwg5KSknTp0iXFxsbqzJkzzjbDhg3Tt99+qy+//FJr1qzR0aNH9fjjjxfZ7/W810vb1eZ69OhRHT16VG+//bZ27dqlmTNnaunSperbt+9V+y7ua79UGdwwo0ePNk2aNLnm9k8++aRp166dS1lUVJQZOHBgCY/sxhg6dKipU6eOyc3NLbB+xowZxs/P78YOqgRIMvPnz3eu5+bmmpCQEDNx4kRnWWZmpvH09DSfffZZof3cd999Jj4+3rmek5NjQkNDTWJiYqmM+3pcOdeCbNq0yUgyhw8fLrRNcd8LZaWg+fbq1ct06NChWP2Ul2PboUMH89BDDxXZxirH9tixY0aSWbNmjTHmj/dohQoVzJdffulss3fvXiPJJCcnF9jH9b7Xb7Qr51qQL774wtjtdnPp0qVC21zPa780cQbmBtu3b59CQ0NVu3Ztde/eXUeOHCm0bXJysmJiYlzK4uLilJycXNrDLHEXL17Uf/7zH/Xp06fIL9c8ffq0wsPDFRYWpg4dOmj37t03cJQl4+DBg0pPT3c5dn5+foqKiir02F28eFFbt2512cbNzU0xMTGWO95ZWVmy2WxX/S6y4rwXbjarV69WUFCQ6tatq+eee04nTpwotG15ObYZGRlatGjRNf0v3QrHNu9ySdWqVSVJW7du1aVLl1yOU7169VSzZs1Cj9P1vNfLwpVzLayNr6+vPDyK/nzb4rz2SxsB5gaKiopynqqbOnWqDh48qGbNmunUqVMFtk9PT8/31QnBwcFKT0+/EcMtUd98840yMzPVu3fvQtvUrVtX06dP14IFC/Sf//xHubm5+tvf/qZff/31xg20BOQdn+Icu99++005OTmWP97nz5/XiBEj1LVr1yK/EK6474WbSevWrTVr1iytWLFC//znP7VmzRq1adNGOTk5BbYvL8f2k08+kY+Pz1UvqVjh2Obm5uqFF17QAw88oDvvvFPSH+9bu92eL3gXdZyu571+oxU01yv99ttveuONNzRgwIAi+yrua7+03dRfJVDetGnTxvlz48aNFRUVpfDwcH3xxRfX9L8aK/v444/Vpk2bIr8ePTo6WtHR0c71v/3tb6pfv74+/PBDvfHGGzdimPgTLl26pCeffFLGGE2dOrXItlZ+L3Tp0sX5c6NGjdS4cWPVqVNHq1evVqtWrcpwZKVr+vTp6t69+1VvrLfCsY2Pj9euXbtuintzStvV5upwONSuXTs1aNBACQkJRfZ1s732OQNThvz9/XXHHXdo//79BdaHhITkuwM+IyNDISEhN2J4Jebw4cNavny5+vXrV6ztKlSooLvvvrvQ38/NKu/4FOfYVatWTe7u7pY93nnh5fDhw0pKSiry7EtBrvZeuJnVrl1b1apVK3TsVj+2kvT9998rNTW12O9h6eY7toMHD9bChQu1atUq1ahRw1keEhKiixcvKjMz06V9Ucfpet7rN1Jhc81z6tQptW7dWj4+Ppo/f74qVKhQrP6v9tovbQSYMnT69GkdOHBA1atXL7A+OjpaK1ascClLSkpyOUthBTNmzFBQUJDatWtXrO1ycnK0c+fOQn8/N6uIiAiFhIS4HDuHw6GNGzcWeuzsdrsiIyNdtsnNzdWKFStu+uOdF1727dun5cuXKyAgoNh9XO29cDP79ddfdeLEiULHbuVjm+fjjz9WZGSkmjRpUuxtb5Zja4zR4MGDNX/+fK1cuVIREREu9ZGRkapQoYLLcUpNTdWRI0cKPU7X816/Ea42V+mPccbGxsput+u///3vdX1kxdVe+6WujG8ivqW8+OKLZvXq1ebgwYNm/fr1JiYmxlSrVs0cO3bMGGPM008/bV555RVn+/Xr1xsPDw/z9ttvm71795rRo0ebChUqmJ07d5bVFIotJyfH1KxZ04wYMSJf3ZXzHTNmjPnuu+/MgQMHzNatW02XLl2Ml5eX2b17940c8jU5deqU+fHHH82PP/5oJJlJkyaZH3/80fnkzfjx442/v79ZsGCB2bFjh+nQoYOJiIgw586dc/bx0EMPmcmTJzvXP//8c+Pp6Wlmzpxp9uzZYwYMGGD8/f1Nenr6DZ/f5Yqa68WLF82jjz5qatSoYVJSUkxaWppzuXDhgrOPK+d6tfdCWSpqvqdOnTIvvfSSSU5ONgcPHjTLly8399xzj7n99tvN+fPnnX2Uh2ObJysry3h7e5upU6cW2IdVju1zzz1n/Pz8zOrVq11ep2fPnnW2efbZZ03NmjXNypUrzZYtW0x0dLSJjo526adu3bpm3rx5zvVrea/faFeba1ZWlomKijKNGjUy+/fvd2mTnZ3t7OfyuV7ra/9GIsDcQE899ZSpXr26sdvt5i9/+Yt56qmnzP79+531Dz74oOnVq5fLNl988YW54447jN1uNw0bNjSLFi26waP+c7777jsjyaSmpuaru3K+L7zwgqlZs6ax2+0mODjYtG3b1mzbtu0GjvbarVq1ykjKt+TNJzc317z++usmODjYeHp6mlatWuX7HYSHh5vRo0e7lE2ePNn5O7jvvvvMDz/8cINmVLii5nrw4MEC6ySZVatWOfu4cq5Xey+UpaLme/bsWRMbG2sCAwNNhQoVTHh4uOnfv3++IFIejm2eDz/80FSsWNFkZmYW2IdVjm1hr9MZM2Y425w7d84MGjTIVKlSxXh7e5vHHnvMpKWl5evn8m2u5b1+o11troUdd0nm4MGDLv3kbXOtr/0byfb/DxIAAMAyuAcGAABYDgEGAABYDgEGAABYDgEGAABYDgEGAABYDgEGAABYDgEGAABYDgEGuEX07t1bHTt2LNMxpKen6+GHH1alSpXyfetvaSuL+deqVUvvvvvuDd0ncKvg26iBW8R7772nsv7cynfeeUdpaWlKSUmRn59fmY4FgLURYIBbxM0QGA4cOKDIyEjdfvvtZT0UABbHJSSgHPnqq6/UqFEjVaxYUQEBAYqJidGZM2ckuV5COXTokGw2W76lRYsWzr7WrVunZs2aqWLFigoLC9Pzzz/v7KswU6dOVZ06dWS321W3bl19+umnzrpatWrp66+/1qxZs2Sz2dS7d+8C+8gb51tvvaXg4GD5+/tr7Nixys7O1ssvv6yqVauqRo0amjFjhst2O3fu1EMPPeSc+4ABA3T69OlCx5qbm6vExERFRESoYsWKatKkib766iuXNrt379YjjzwiX19f+fj4qFmzZjpw4IAkqUWLFnrhhRdc2nfs2LHQeUlSZmam+vXrp8DAQPn6+uqhhx7S9u3bnfXbt29Xy5Yt5ePjI19fX0VGRmrLli2F9gfcyggwQDmRlpamrl27qk+fPtq7d69Wr16txx9/vMDLRmFhYUpLS3MuP/74owICAtS8eXNJf5wpad26tTp16qQdO3Zo7ty5WrdunQYPHlzo/ufPn6+hQ4fqxRdf1K5duzRw4EA988wzWrVqlSRp8+bNat26tZ588kmlpaXpvffeK7SvlStX6ujRo1q7dq0mTZqk0aNH65FHHlGVKlW0ceNGPfvssxo4cKB+/fVXSdKZM2cUFxenKlWqaPPmzfryyy+1fPnyIsebmJioWbNmadq0adq9e7eGDRumHj16aM2aNZKk//f//p+aN28uT09PrVy5Ulu3blWfPn2UnZ199YNRiM6dO+vYsWNasmSJtm7dqnvuuUetWrXSyZMnJUndu3dXjRo1tHnzZm3dulWvvPKKKlSocN37A8q1MvsaSQAlauvWrUaSOXToUIH1vXr1Mh06dMhXfu7cORMVFWUeeeQRk5OTY4wxpm/fvmbAgAEu7b7//nvj5uZmzp07V2D/f/vb30z//v1dyjp37mzatm3rXO/QoUO+b1wvaJzh4eHOsRhjTN26dU2zZs2c69nZ2aZSpUrms88+M8YY89FHH5kqVaqY06dPO9ssWrTIuLm5Ob8t9/L5nz9/3nh7e5sNGza47Ltv376ma9euxhhjRo4caSIiIszFixcLHOeDDz5ohg4d6lJ25fzCw8PNO++8Y4z54/fn6+trzp8/77JNnTp1zIcffmiMMcbHx8fMnDmzqF8PgP8fZ2CAcqJJkyZq1aqVGjVqpM6dO+v//J//o99///2q2/Xp00enTp3SnDlz5Ob2xz8J27dv18yZM1W5cmXnEhcXp9zcXB08eLDAfvbu3asHHnjApeyBBx7Q3r17iz2Xhg0bOsciScHBwWrUqJFz3d3dXQEBATp27Jhz302aNFGlSpVc9p2bm6vU1NR8/e/fv19nz57Vww8/7DLHWbNmOS8RpaSkqFmzZiV2BmT79u06ffq0AgICXPZ58OBB5z6HDx+ufv36KSYmRuPHj3eWA8iPm3iBcsLd3V1JSUnasGGDli1bpsmTJ+vVV1/Vxo0bFRERUeA248aN03fffadNmzbJx8fHWX769GkNHDhQzz//fL5tatasWWpzyHNlaLDZbAWW5ebmXlf/effGLFq0SH/5y19c6jw9PSVJFStWLLIPNze3fJfnLl26VOQ+q1evrtWrV+ery3ukPCEhQd26ddOiRYu0ZMkSjR49Wp9//rkee+yxq00JuOVwBgYoR2w2mx544AGNGTNGP/74o+x2u+bPn19g26+//lpjx47VF198oTp16rjU3XPPPdqzZ49uu+22fIvdbi+wv/r162v9+vUuZevXr1eDBg1KZnJFqF+/vrZv3+5yk/H69evl5uamunXr5mvfoEEDeXp66siRI/nmFxYWJklq3Lixvv/++0JDSWBgoNLS0pzrOTk52rVrV6FjvOeee5Seni4PD498+6xWrZqz3R133KFhw4Zp2bJlevzxx/PdrAzgDwQYoJzYuHGj3nrrLW3ZskVHjhzRvHnzdPz4cdWvXz9f2127dqlnz54aMWKEGjZsqPT0dKWnpztvJh0xYoQ2bNigwYMHKyUlRfv27dOCBQuKvCn25Zdf1syZMzV16lTt27dPkyZN0rx58/TSSy+V2pzzdO/eXV5eXurVq5d27dqlVatWaciQIXr66acVHBycr72Pj49eeuklDRs2TJ988okOHDigbdu2afLkyfrkk08kSYMHD5bD4VCXLl20ZcsW7du3T59++qnzktRDDz2kRYsWadGiRfrpp5/03HPPKTMzs9AxxsTEKDo6Wh07dtSyZct06NAhbdiwQa+++qq2bNmic+fOafDgwVq9erUOHz6s9evXa/PmzQUePwBcQgLKDV9fX61du1bvvvuuHA6HwsPD9a9//Utt2rTJ13bLli06e/asxo0bp3HjxjnLH3zwQa1evVqNGzfWmjVr9Oqrr6pZs2YyxqhOnTp66qmnCt1/x44d9d577+ntt9/W0KFDFRERoRkzZrg8ml1avL299d1332no0KG699575e3trU6dOmnSpEmFbvPGG28oMDBQiYmJ+vnnn+Xv76977rlH//jHPyRJAQEBWrlypV5++WU9+OCDcnd311133eW8z6dPnz7avn27evbsKQ8PDw0bNkwtW7YsdH82m02LFy/Wq6++qmeeeUbHjx9XSEiImjdvruDgYLm7u+vEiRPq2bOnMjIyVK1aNT3++OMaM2ZMyf6ygHLCZq68iAsAAHCT4xISAACwHAIMAACwHAIMAACwHAIMAACwHAIMAACwHAIMAACwHAIMAACwHAIMAACwHAIMAACwHAIMAACwHAIMAACwHAIMAACwnP8P6QHA1LikLtkAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "- Invariant feature map/extraction: $X \\to (\\{f^d \\}, \\{f^o \\}, f^*)$\n",
        "- Learn MLP(deep_set_1( $\\{f^d \\}$ , deep_set_2($\\{f^d \\}$), MLP_s($f^*$)), where\n",
        "  - deep_set_i, MLP_s: input_dim = 1, output_dim = hid_dim\n",
        "  - MLP: input_dim = hid_dim + hid_dim + hid_dim, output_dim = num_classes (invariant classification) / 1 (invariant scalar regression)\n",
        "- Deepset, proposed in [Zaheer et al. ](https://arxiv.org/pdf/1703.06114.pdf), can universally approximate continuous functions on sets (where elements are from countable domain), by choosing suitable $\\rho, \\phi$, and maps a input set $X$ via\n",
        "$$\n",
        "\\rho ( \\sum_{x \\in X} \\phi(x) )\n",
        "$$\n",
        "We use the Deepset implementation in Pytorch Geometric, which can apply to batch of sets\n",
        "\n",
        "WHY ARE WE BETTER?\n",
        "- using Deepset to deal with the varying-size problem (not ad-hoc zero padding), which also minimizes memory storage and much lightweight\n",
        "- only using 100 binary expansion (basis), also comparatively lightweight (few) compared to SOTA while giving good results\n",
        "- [TBC] few-shot learning / low sample complexity"
      ],
      "metadata": {
        "id": "wVnZ53JAWJzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScalarModel(torch.nn.Module):\n",
        "    def __init__(self, hid_dim, out_dim, dropout=0, input_dim=1):\n",
        "        super(ScalarModel, self).__init__()\n",
        "        #Deepset for diagonal scalars\n",
        "        phi_d = MLP([input_dim,hid_dim, hid_dim*2], dropout=[dropout]*2)\n",
        "        rho_d = MLP([hid_dim*2, hid_dim], dropout=[dropout]*1) #MLP([hid_dim*2, hid_dim, hid_dim], dropout=[dropout]*2) #\n",
        "        self.deepset_d = DeepSetsAggregation(local_nn=phi_d, global_nn=rho_d)\n",
        "        #Deepset for off-diagonal scalars\n",
        "        phi_o = MLP([input_dim,hid_dim, hid_dim*2], dropout=[dropout]*2)\n",
        "        rho_o = MLP([hid_dim*2, hid_dim], dropout=[dropout]*1) #MLP([hid_dim*2, hid_dim, hid_dim], dropout=[dropout]*2) \n",
        "        self.deepset_o = DeepSetsAggregation(local_nn=phi_o, global_nn=rho_o) \n",
        "        #MLP_s for f_star\n",
        "        self.MLP_s = MLP([input_dim,hid_dim, hid_dim], dropout=[dropout]*2)  #MLP([1,hid_dim*2, hid_dim], dropout=[dropout]*2) #\n",
        "        #MLP for (Deepset(f_d), Deepset(f_o), f_star)\n",
        "        self.MLP_out = MLP([hid_dim*3, hid_dim, out_dim], dropout=[dropout]*2)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for net in [self.deepset_d, self.deepset_o, self.MLP_s, self.MLP_out]:\n",
        "        #for net in [self.deepset_d, self.deepset_o, self.MLP_out]:\n",
        "            net.reset_parameters()\n",
        "    \n",
        "    def forward(self, data):\n",
        "        out_d = self.deepset_d(data.f_d, data.f_d_batch) # bs x hid_dim\n",
        "        out_o = self.deepset_o(data.f_o, data.f_o_batch) # bs x hid_dim\n",
        "        out_star = self.MLP_s(data.f_star) #bs x hid_dim\n",
        "        #concat and output final embedding\n",
        "        out = self.MLP_out(torch.concat([out_d, out_o, out_star], dim=-1)) # bs x hid_dim*3 -> bs x out_dim\n",
        "        return out"
      ],
      "metadata": {
        "id": "-up-LvBGQQE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "L8o9Z-S0WWON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code adapated from https://github.com/nhuang37/untrain_MPNN/blob/main/GNN_untrained/auxiliarymethods/gnn_evaluation.py\n",
        "\n",
        "def train(train_loader, model, optimizer, device):\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "\n",
        "    lf = torch.nn.L1Loss()\n",
        "\n",
        "    for data in train_loader:\n",
        "        #print(data)\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = lf(model(data), data.y)\n",
        "\n",
        "        loss.backward()\n",
        "        loss_all += loss.item() * data.num_graphs\n",
        "        optimizer.step()\n",
        "    return (loss_all / len(train_loader.dataset))\n",
        "\n",
        "\n",
        "def test(loader, model, device, per_target=False):\n",
        "    model.eval()\n",
        "    out_dim = model.MLP_out.channel_list[-1]\n",
        "    if per_target:\n",
        "      error_pt = torch.zeros(out_dim)\n",
        "    error = 0\n",
        "\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        error_multi = (model(data) - data.y).abs()\n",
        "        if per_target:\n",
        "          error_pt += error_multi.sum(axis=0).cpu()\n",
        "\n",
        "        error += error_multi.sum().item()\n",
        "    if per_target:\n",
        "      return error / len(loader.dataset), error_pt / len(loader.dataset)\n",
        "    else:\n",
        "      return error / len(loader.dataset), None\n",
        "\n",
        "\n",
        "# 10-CV for NN training and hyperparameter selection.\n",
        "def nn_evaluation(dataset, hid_dim, out_dim, max_num_epochs=200, batch_size=128, start_lr=0.01, min_lr = 0.000001, factor=0.5, patience=50,\n",
        "                       num_repetitions=5, all_std=True, verbose=True, dropout=0, per_target=False):\n",
        "    #reproducibility\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # Set device.\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    test_all = []\n",
        "    test_complete = []\n",
        "\n",
        "    # Input dim\n",
        "    input_dim = dataset.data.f_star.shape[1]\n",
        "    print(input_dim)\n",
        "\n",
        "    for i in range(num_repetitions):\n",
        "        # Test acc. over all folds.\n",
        "        test_error = []\n",
        "        kf = KFold(n_splits=10, shuffle=True, random_state=i)\n",
        "        #dataset.shuffle()\n",
        "\n",
        "        for train_index, test_index in kf.split(list(range(len(dataset)))):\n",
        "            # Sample 10% split from training split for validation.\n",
        "            train_index, val_index = train_test_split(train_index, test_size=0.1, random_state=i)\n",
        "            best_val_error = None\n",
        "            best_test = None\n",
        "\n",
        "            # Split data.\n",
        "            train_dataset = dataset[train_index.tolist()]\n",
        "            val_dataset = dataset[val_index.tolist()]\n",
        "            test_dataset = dataset[test_index.tolist()]\n",
        "\n",
        "            # Prepare batching. (follow_batch: batchify both f_d, f_o)\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, follow_batch=['f_d', 'f_o'])\n",
        "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, follow_batch=['f_d', 'f_o'])\n",
        "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, follow_batch=['f_d', 'f_o']) #reproducible\n",
        "\n",
        "            # Collect val. and test acc. over all hyperparameter combinations.\n",
        "            # Setup model.\n",
        "            model = ScalarModel(hid_dim, out_dim, dropout, input_dim).to(device)\n",
        "            #model = ScalarModel_Bessel(hid_dim, out_dim, dropout).to(device)\n",
        "            model.reset_parameters()\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=start_lr)\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                                    factor=factor, patience=patience,\n",
        "                                                                    min_lr=0.0000001)\n",
        "            for epoch in range(1, max_num_epochs + 1):\n",
        "                lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "                loss = train(train_loader, model, optimizer, device)\n",
        "                val_error, val_pt = test(val_loader, model, device, per_target)\n",
        "                scheduler.step(val_error)\n",
        "\n",
        "                if best_val_error is None or val_error <= best_val_error:\n",
        "                    best_val_error = val_error\n",
        "                    best_test, test_pt = test(test_loader, model, device, per_target)\n",
        "                if verbose and (epoch+1) % 50 == 0:\n",
        "                    print('Epoch: {:03d}, LR: {:.7f}, Loss: {:.4f},'\n",
        "                      'Val MAE: {:.4f}'.format(epoch, lr, loss, val_error))\n",
        "                # Break if learning rate is smaller 10**-6.\n",
        "                if lr < min_lr:\n",
        "                    break\n",
        "            if per_target:\n",
        "                print(f\"per_target Val MAE = {val_pt}\")\n",
        "            test_error.append(best_test)\n",
        "            print(f\"Finish run with best_test={best_test:.4f}\")\n",
        "            if all_std:\n",
        "                test_complete.append(best_test)\n",
        "        test_all.append(float(np.array(test_error).mean()))\n",
        "\n",
        "    if all_std:\n",
        "        return (np.array(test_all).mean(), np.array(test_all).std(),\n",
        "                np.array(test_complete).std())\n",
        "    else:\n",
        "        return (np.array(test_all).mean(), np.array(test_all).std())"
      ],
      "metadata": {
        "id": "d2ssWTYuWXO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Note: the basis transform scales linearly with the number of basis"
      ],
      "metadata": {
        "id": "kvOg9Nq4l8o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hid_dim = 512\n",
        "out_dim = 1 #14\n",
        "dropout = 0\n",
        "batch_size = 128\n",
        "max_num_epochs = 1000\n",
        "min_lr = 0.0000001 #UPDATED\n",
        "patience = 20 #UPDATED\n",
        "mae, std, std_all = nn_evaluation(dataset, hid_dim, out_dim, batch_size=batch_size, max_num_epochs=max_num_epochs, \n",
        "                                  start_lr=0.02, min_lr = min_lr, factor=0.8, patience=patience,\n",
        "                       num_repetitions=1, all_std=True, verbose=True, dropout=dropout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXsuuVj2jrNZ",
        "outputId": "aadf4dd4-d08f-48a8-a594-fba69d8bea1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "Epoch: 049, LR: 0.0200000, Loss: 25.3624,Val MAE: 14.6548\n",
            "Epoch: 099, LR: 0.0128000, Loss: 24.1583,Val MAE: 132.6890\n",
            "Epoch: 149, LR: 0.0081920, Loss: 25.1029,Val MAE: 78.9177\n",
            "Epoch: 199, LR: 0.0052429, Loss: 20.9481,Val MAE: 21.1522\n",
            "Epoch: 249, LR: 0.0033554, Loss: 20.9012,Val MAE: 31.1724\n",
            "Epoch: 299, LR: 0.0021475, Loss: 19.0874,Val MAE: 16.5821\n",
            "Epoch: 349, LR: 0.0013744, Loss: 19.5101,Val MAE: 19.3185\n",
            "Epoch: 399, LR: 0.0010995, Loss: 20.3932,Val MAE: 14.3973\n",
            "Epoch: 449, LR: 0.0007037, Loss: 19.4771,Val MAE: 13.6790\n",
            "Epoch: 499, LR: 0.0004504, Loss: 17.5787,Val MAE: 26.0331\n",
            "Epoch: 549, LR: 0.0002882, Loss: 21.3642,Val MAE: 10.7732\n",
            "Epoch: 599, LR: 0.0001476, Loss: 19.0149,Val MAE: 8.4480\n",
            "Epoch: 649, LR: 0.0000944, Loss: 20.0332,Val MAE: 9.5044\n",
            "Epoch: 699, LR: 0.0000604, Loss: 17.6751,Val MAE: 8.5911\n",
            "Epoch: 749, LR: 0.0000484, Loss: 20.8252,Val MAE: 8.1631\n",
            "Epoch: 799, LR: 0.0000248, Loss: 20.3293,Val MAE: 7.9869\n",
            "Epoch: 849, LR: 0.0000158, Loss: 18.4630,Val MAE: 7.2925\n",
            "Epoch: 899, LR: 0.0000101, Loss: 20.8485,Val MAE: 8.7127\n",
            "Epoch: 949, LR: 0.0000065, Loss: 22.0089,Val MAE: 7.3627\n",
            "Epoch: 999, LR: 0.0000033, Loss: 20.2783,Val MAE: 7.6808\n",
            "Finish run with best_test=7.0370\n",
            "Epoch: 049, LR: 0.0160000, Loss: 25.3211,Val MAE: 561.9748\n",
            "Epoch: 099, LR: 0.0102400, Loss: 23.5713,Val MAE: 185.0672\n",
            "Epoch: 149, LR: 0.0052429, Loss: 23.8981,Val MAE: 75.8775\n",
            "Epoch: 199, LR: 0.0041943, Loss: 21.3440,Val MAE: 26.7986\n",
            "Epoch: 249, LR: 0.0026844, Loss: 24.3484,Val MAE: 249.1671\n",
            "Epoch: 299, LR: 0.0017180, Loss: 20.5426,Val MAE: 14.8116\n",
            "Epoch: 349, LR: 0.0008796, Loss: 22.5500,Val MAE: 41.8447\n",
            "Epoch: 399, LR: 0.0005629, Loss: 18.0172,Val MAE: 40.8702\n",
            "Epoch: 449, LR: 0.0003603, Loss: 19.6528,Val MAE: 30.3071\n",
            "Epoch: 499, LR: 0.0002306, Loss: 18.1803,Val MAE: 18.1334\n",
            "Epoch: 549, LR: 0.0001476, Loss: 18.3769,Val MAE: 7.8803\n",
            "Epoch: 599, LR: 0.0000944, Loss: 20.2427,Val MAE: 9.4590\n",
            "Epoch: 649, LR: 0.0000756, Loss: 20.2861,Val MAE: 8.2356\n",
            "Epoch: 699, LR: 0.0000387, Loss: 18.8108,Val MAE: 7.9803\n",
            "Epoch: 749, LR: 0.0000309, Loss: 20.6739,Val MAE: 8.9743\n",
            "Epoch: 799, LR: 0.0000158, Loss: 20.8738,Val MAE: 9.1349\n",
            "Epoch: 849, LR: 0.0000101, Loss: 20.9114,Val MAE: 7.5317\n",
            "Epoch: 899, LR: 0.0000065, Loss: 21.7900,Val MAE: 8.4259\n",
            "Epoch: 949, LR: 0.0000042, Loss: 21.4055,Val MAE: 7.7168\n",
            "Epoch: 999, LR: 0.0000027, Loss: 19.9202,Val MAE: 9.3541\n",
            "Finish run with best_test=7.8780\n",
            "Epoch: 049, LR: 0.0160000, Loss: 28.0618,Val MAE: 149.2925\n",
            "Epoch: 099, LR: 0.0102400, Loss: 26.9068,Val MAE: 66.5560\n",
            "Epoch: 149, LR: 0.0065536, Loss: 22.9923,Val MAE: 20.5945\n",
            "Epoch: 199, LR: 0.0041943, Loss: 22.1058,Val MAE: 24.1003\n",
            "Epoch: 249, LR: 0.0033554, Loss: 22.4210,Val MAE: 24.4251\n",
            "Epoch: 299, LR: 0.0021475, Loss: 23.8934,Val MAE: 52.5350\n",
            "Epoch: 349, LR: 0.0013744, Loss: 22.0819,Val MAE: 39.1420\n",
            "Epoch: 399, LR: 0.0008796, Loss: 19.8964,Val MAE: 54.6970\n",
            "Epoch: 449, LR: 0.0005629, Loss: 23.5469,Val MAE: 59.3372\n",
            "Epoch: 499, LR: 0.0003603, Loss: 19.9654,Val MAE: 20.7708\n",
            "Epoch: 549, LR: 0.0002882, Loss: 20.5241,Val MAE: 16.1352\n",
            "Epoch: 599, LR: 0.0001845, Loss: 18.1129,Val MAE: 16.2414\n",
            "Epoch: 649, LR: 0.0001181, Loss: 18.7551,Val MAE: 15.0138\n",
            "Epoch: 699, LR: 0.0000756, Loss: 19.1045,Val MAE: 7.9910\n",
            "Epoch: 749, LR: 0.0000484, Loss: 19.1945,Val MAE: 7.8015\n",
            "Epoch: 799, LR: 0.0000248, Loss: 20.8961,Val MAE: 8.9224\n",
            "Epoch: 849, LR: 0.0000158, Loss: 19.7893,Val MAE: 9.2607\n",
            "Epoch: 899, LR: 0.0000127, Loss: 19.7030,Val MAE: 7.8351\n",
            "Epoch: 949, LR: 0.0000065, Loss: 19.5239,Val MAE: 8.0047\n",
            "Epoch: 999, LR: 0.0000042, Loss: 22.0373,Val MAE: 8.0218\n",
            "Finish run with best_test=7.5479\n",
            "Epoch: 049, LR: 0.0160000, Loss: 27.8696,Val MAE: 56.4467\n",
            "Epoch: 099, LR: 0.0102400, Loss: 30.2310,Val MAE: 67.1815\n",
            "Epoch: 149, LR: 0.0065536, Loss: 26.0517,Val MAE: 307.2936\n",
            "Epoch: 199, LR: 0.0052429, Loss: 22.5158,Val MAE: 85.4766\n",
            "Epoch: 249, LR: 0.0026844, Loss: 21.7406,Val MAE: 18.0731\n",
            "Epoch: 299, LR: 0.0017180, Loss: 18.9896,Val MAE: 26.3433\n",
            "Epoch: 349, LR: 0.0010995, Loss: 20.4990,Val MAE: 46.6012\n",
            "Epoch: 399, LR: 0.0008796, Loss: 19.1231,Val MAE: 13.9010\n",
            "Epoch: 449, LR: 0.0004504, Loss: 22.9130,Val MAE: 17.6252\n",
            "Epoch: 499, LR: 0.0002882, Loss: 18.9469,Val MAE: 15.2362\n",
            "Epoch: 549, LR: 0.0002306, Loss: 19.8771,Val MAE: 10.6706\n",
            "Epoch: 599, LR: 0.0001476, Loss: 18.6019,Val MAE: 8.1080\n",
            "Epoch: 649, LR: 0.0000944, Loss: 20.2243,Val MAE: 10.4427\n",
            "Epoch: 699, LR: 0.0000604, Loss: 19.0569,Val MAE: 8.0769\n",
            "Epoch: 749, LR: 0.0000484, Loss: 19.5460,Val MAE: 9.0914\n",
            "Epoch: 799, LR: 0.0000248, Loss: 18.5339,Val MAE: 8.8315\n",
            "Epoch: 849, LR: 0.0000158, Loss: 19.3045,Val MAE: 9.0461\n",
            "Epoch: 899, LR: 0.0000127, Loss: 20.9645,Val MAE: 8.7770\n",
            "Epoch: 949, LR: 0.0000065, Loss: 20.2601,Val MAE: 8.3723\n",
            "Epoch: 999, LR: 0.0000042, Loss: 20.6099,Val MAE: 9.5331\n",
            "Finish run with best_test=7.9341\n",
            "Epoch: 049, LR: 0.0160000, Loss: 26.6640,Val MAE: 92.9398\n",
            "Epoch: 099, LR: 0.0102400, Loss: 25.0752,Val MAE: 102.3137\n",
            "Epoch: 149, LR: 0.0065536, Loss: 23.2687,Val MAE: 17.4222\n",
            "Epoch: 199, LR: 0.0041943, Loss: 24.6863,Val MAE: 99.2722\n",
            "Epoch: 249, LR: 0.0026844, Loss: 23.2112,Val MAE: 24.2121\n",
            "Epoch: 299, LR: 0.0017180, Loss: 23.1529,Val MAE: 48.7672\n",
            "Epoch: 349, LR: 0.0013744, Loss: 19.4841,Val MAE: 55.3509\n",
            "Epoch: 399, LR: 0.0008796, Loss: 19.1696,Val MAE: 8.5050\n",
            "Epoch: 449, LR: 0.0005629, Loss: 21.1513,Val MAE: 36.9317\n",
            "Epoch: 499, LR: 0.0003603, Loss: 18.5005,Val MAE: 8.9730\n",
            "Epoch: 549, LR: 0.0002306, Loss: 19.1482,Val MAE: 8.1996\n",
            "Epoch: 599, LR: 0.0001476, Loss: 20.3363,Val MAE: 16.6877\n",
            "Epoch: 649, LR: 0.0000944, Loss: 21.4589,Val MAE: 11.1265\n",
            "Epoch: 699, LR: 0.0000604, Loss: 19.2443,Val MAE: 8.0185\n",
            "Epoch: 749, LR: 0.0000387, Loss: 21.8687,Val MAE: 11.0045\n",
            "Epoch: 799, LR: 0.0000248, Loss: 21.2468,Val MAE: 7.6316\n",
            "Epoch: 849, LR: 0.0000158, Loss: 18.3501,Val MAE: 7.9971\n",
            "Epoch: 899, LR: 0.0000101, Loss: 19.9303,Val MAE: 9.9908\n",
            "Epoch: 949, LR: 0.0000065, Loss: 19.3022,Val MAE: 8.2334\n",
            "Epoch: 999, LR: 0.0000042, Loss: 21.7378,Val MAE: 8.7450\n",
            "Finish run with best_test=7.9065\n",
            "Epoch: 049, LR: 0.0160000, Loss: 23.4980,Val MAE: 353.2211\n",
            "Epoch: 099, LR: 0.0102400, Loss: 21.8567,Val MAE: 51.4939\n",
            "Epoch: 149, LR: 0.0065536, Loss: 24.2860,Val MAE: 86.9503\n",
            "Epoch: 199, LR: 0.0041943, Loss: 22.1372,Val MAE: 195.7241\n",
            "Epoch: 249, LR: 0.0026844, Loss: 22.3033,Val MAE: 107.3274\n",
            "Epoch: 299, LR: 0.0013744, Loss: 18.9557,Val MAE: 105.2244\n",
            "Epoch: 349, LR: 0.0010995, Loss: 19.6271,Val MAE: 87.2176\n",
            "Epoch: 399, LR: 0.0008796, Loss: 21.5095,Val MAE: 17.2810\n",
            "Epoch: 449, LR: 0.0004504, Loss: 20.5142,Val MAE: 41.6714\n",
            "Epoch: 499, LR: 0.0002882, Loss: 18.0278,Val MAE: 16.4171\n",
            "Epoch: 549, LR: 0.0002306, Loss: 19.0423,Val MAE: 32.3839\n",
            "Epoch: 599, LR: 0.0001476, Loss: 19.8146,Val MAE: 9.5371\n",
            "Epoch: 649, LR: 0.0000944, Loss: 17.4075,Val MAE: 15.0821\n",
            "Epoch: 699, LR: 0.0000604, Loss: 20.9291,Val MAE: 8.1503\n",
            "Epoch: 749, LR: 0.0000604, Loss: 19.0286,Val MAE: 7.9368\n",
            "Epoch: 799, LR: 0.0000387, Loss: 21.3638,Val MAE: 8.8448\n",
            "Epoch: 849, LR: 0.0000248, Loss: 20.7581,Val MAE: 8.8029\n",
            "Epoch: 899, LR: 0.0000158, Loss: 18.9763,Val MAE: 9.8630\n",
            "Epoch: 949, LR: 0.0000101, Loss: 20.1954,Val MAE: 8.4302\n",
            "Epoch: 999, LR: 0.0000052, Loss: 20.9075,Val MAE: 8.1749\n",
            "Finish run with best_test=7.6079\n",
            "Epoch: 049, LR: 0.0160000, Loss: 29.4866,Val MAE: 74.6634\n",
            "Epoch: 099, LR: 0.0128000, Loss: 25.3387,Val MAE: 395.6588\n",
            "Epoch: 149, LR: 0.0081920, Loss: 25.7322,Val MAE: 193.3247\n",
            "Epoch: 199, LR: 0.0065536, Loss: 21.2518,Val MAE: 310.4183\n",
            "Epoch: 249, LR: 0.0033554, Loss: 20.2408,Val MAE: 26.9435\n",
            "Epoch: 299, LR: 0.0021475, Loss: 17.9785,Val MAE: 33.6007\n",
            "Epoch: 349, LR: 0.0013744, Loss: 19.1660,Val MAE: 37.0492\n",
            "Epoch: 399, LR: 0.0008796, Loss: 22.7525,Val MAE: 40.3011\n",
            "Epoch: 449, LR: 0.0005629, Loss: 20.1270,Val MAE: 19.0977\n",
            "Epoch: 499, LR: 0.0003603, Loss: 20.4809,Val MAE: 13.8762\n",
            "Epoch: 549, LR: 0.0002882, Loss: 21.7140,Val MAE: 28.9861\n",
            "Epoch: 599, LR: 0.0001845, Loss: 19.3414,Val MAE: 8.1237\n",
            "Epoch: 649, LR: 0.0001476, Loss: 19.6067,Val MAE: 8.0649\n",
            "Epoch: 699, LR: 0.0000944, Loss: 19.8780,Val MAE: 8.1678\n",
            "Epoch: 749, LR: 0.0000484, Loss: 19.1517,Val MAE: 7.9852\n",
            "Epoch: 799, LR: 0.0000309, Loss: 18.2342,Val MAE: 7.7803\n",
            "Epoch: 849, LR: 0.0000198, Loss: 20.2875,Val MAE: 7.5591\n",
            "Epoch: 899, LR: 0.0000101, Loss: 20.6305,Val MAE: 9.2781\n",
            "Epoch: 949, LR: 0.0000065, Loss: 25.3392,Val MAE: 8.5591\n",
            "Epoch: 999, LR: 0.0000042, Loss: 19.3557,Val MAE: 7.5984\n",
            "Finish run with best_test=7.9557\n",
            "Epoch: 049, LR: 0.0160000, Loss: 25.2253,Val MAE: 136.1600\n",
            "Epoch: 099, LR: 0.0102400, Loss: 29.8848,Val MAE: 221.8059\n",
            "Epoch: 149, LR: 0.0065536, Loss: 23.2185,Val MAE: 228.5570\n",
            "Epoch: 199, LR: 0.0041943, Loss: 19.8219,Val MAE: 118.6330\n",
            "Epoch: 249, LR: 0.0021475, Loss: 19.7505,Val MAE: 30.6058\n",
            "Epoch: 299, LR: 0.0013744, Loss: 19.8277,Val MAE: 75.0426\n",
            "Epoch: 349, LR: 0.0008796, Loss: 19.1872,Val MAE: 8.2120\n",
            "Epoch: 399, LR: 0.0005629, Loss: 18.3655,Val MAE: 10.2303\n",
            "Epoch: 449, LR: 0.0003603, Loss: 20.6938,Val MAE: 20.3686\n",
            "Epoch: 499, LR: 0.0002306, Loss: 19.0580,Val MAE: 13.8898\n",
            "Epoch: 549, LR: 0.0001476, Loss: 19.5998,Val MAE: 11.9645\n",
            "Epoch: 599, LR: 0.0001181, Loss: 20.7272,Val MAE: 11.9058\n",
            "Epoch: 649, LR: 0.0000756, Loss: 20.4490,Val MAE: 9.9730\n",
            "Epoch: 699, LR: 0.0000484, Loss: 19.7605,Val MAE: 11.1438\n",
            "Epoch: 749, LR: 0.0000309, Loss: 18.0659,Val MAE: 7.6219\n",
            "Epoch: 799, LR: 0.0000158, Loss: 19.8583,Val MAE: 7.4067\n",
            "Epoch: 849, LR: 0.0000127, Loss: 20.1923,Val MAE: 9.8667\n",
            "Epoch: 899, LR: 0.0000065, Loss: 21.9970,Val MAE: 7.7745\n",
            "Epoch: 949, LR: 0.0000042, Loss: 21.8288,Val MAE: 7.4763\n",
            "Epoch: 999, LR: 0.0000021, Loss: 17.0806,Val MAE: 7.7662\n",
            "Finish run with best_test=7.1250\n",
            "Epoch: 049, LR: 0.0160000, Loss: 25.8790,Val MAE: 352.7495\n",
            "Epoch: 099, LR: 0.0081920, Loss: 24.5700,Val MAE: 147.2540\n",
            "Epoch: 149, LR: 0.0052429, Loss: 26.9813,Val MAE: 80.0729\n",
            "Epoch: 199, LR: 0.0041943, Loss: 24.1916,Val MAE: 191.3252\n",
            "Epoch: 249, LR: 0.0026844, Loss: 18.9542,Val MAE: 36.2664\n",
            "Epoch: 299, LR: 0.0017180, Loss: 23.5472,Val MAE: 166.4412\n",
            "Epoch: 349, LR: 0.0013744, Loss: 18.8965,Val MAE: 17.2049\n",
            "Epoch: 399, LR: 0.0007037, Loss: 18.8549,Val MAE: 13.0164\n",
            "Epoch: 449, LR: 0.0004504, Loss: 21.1495,Val MAE: 35.7308\n",
            "Epoch: 499, LR: 0.0003603, Loss: 19.9191,Val MAE: 21.4132\n",
            "Epoch: 549, LR: 0.0002306, Loss: 20.3027,Val MAE: 8.5328\n",
            "Epoch: 599, LR: 0.0001476, Loss: 22.5422,Val MAE: 10.6298\n",
            "Epoch: 649, LR: 0.0000944, Loss: 21.6487,Val MAE: 9.0531\n",
            "Epoch: 699, LR: 0.0000756, Loss: 18.6559,Val MAE: 9.4832\n",
            "Epoch: 749, LR: 0.0000484, Loss: 18.9514,Val MAE: 11.7599\n",
            "Epoch: 799, LR: 0.0000309, Loss: 21.1806,Val MAE: 10.5873\n",
            "Epoch: 849, LR: 0.0000198, Loss: 19.6240,Val MAE: 8.1096\n",
            "Epoch: 899, LR: 0.0000101, Loss: 18.2572,Val MAE: 8.9287\n",
            "Epoch: 949, LR: 0.0000065, Loss: 18.2974,Val MAE: 8.2350\n",
            "Epoch: 999, LR: 0.0000042, Loss: 19.3879,Val MAE: 9.0523\n",
            "Finish run with best_test=8.4274\n",
            "Epoch: 049, LR: 0.0160000, Loss: 27.0027,Val MAE: 61.7472\n",
            "Epoch: 099, LR: 0.0102400, Loss: 27.8734,Val MAE: 390.0071\n",
            "Epoch: 149, LR: 0.0065536, Loss: 22.4393,Val MAE: 407.6548\n",
            "Epoch: 199, LR: 0.0033554, Loss: 21.4412,Val MAE: 43.4831\n",
            "Epoch: 249, LR: 0.0021475, Loss: 20.8804,Val MAE: 122.6817\n",
            "Epoch: 299, LR: 0.0013744, Loss: 18.8734,Val MAE: 129.1660\n",
            "Epoch: 349, LR: 0.0010995, Loss: 21.4757,Val MAE: 87.3765\n",
            "Epoch: 399, LR: 0.0005629, Loss: 19.5997,Val MAE: 76.7844\n",
            "Epoch: 449, LR: 0.0003603, Loss: 20.0199,Val MAE: 11.5932\n",
            "Epoch: 499, LR: 0.0002306, Loss: 19.4681,Val MAE: 21.1203\n",
            "Epoch: 549, LR: 0.0001476, Loss: 17.9656,Val MAE: 9.5975\n",
            "Epoch: 599, LR: 0.0000944, Loss: 18.6458,Val MAE: 16.6102\n",
            "Epoch: 649, LR: 0.0000604, Loss: 20.4473,Val MAE: 12.3555\n",
            "Epoch: 699, LR: 0.0000387, Loss: 21.5538,Val MAE: 14.5072\n",
            "Epoch: 749, LR: 0.0000248, Loss: 19.0261,Val MAE: 11.1523\n",
            "Epoch: 799, LR: 0.0000158, Loss: 20.4833,Val MAE: 9.5900\n",
            "Epoch: 849, LR: 0.0000081, Loss: 18.3372,Val MAE: 7.6552\n",
            "Epoch: 899, LR: 0.0000065, Loss: 21.9883,Val MAE: 7.7118\n",
            "Epoch: 949, LR: 0.0000033, Loss: 19.7834,Val MAE: 7.9875\n",
            "Epoch: 999, LR: 0.0000021, Loss: 22.3811,Val MAE: 8.2304\n",
            "Finish run with best_test=7.1549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hid_dim = 256\n",
        "out_dim = 1 #14\n",
        "dropout = 0\n",
        "batch_size = 128\n",
        "max_num_epochs = 1000\n",
        "min_lr = 0.0000001 #UPDATED\n",
        "patience = 20 #UPDATED\n",
        "mae, std, std_all = nn_evaluation(dataset, hid_dim, out_dim, batch_size=batch_size, max_num_epochs=max_num_epochs, \n",
        "                                  start_lr=0.02, min_lr = min_lr, factor=0.8, patience=patience,\n",
        "                       num_repetitions=1, all_std=True, verbose=True, dropout=dropout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2REu65_xzLN2",
        "outputId": "df328ac1-1698-4b99-f6cb-107d7bf69df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "Epoch: 049, LR: 0.0200000, Loss: 28.6855,Val MAE: 227.5056\n",
            "Epoch: 099, LR: 0.0128000, Loss: 23.7137,Val MAE: 46.0820\n",
            "Epoch: 149, LR: 0.0081920, Loss: 20.4971,Val MAE: 118.2099\n",
            "Epoch: 199, LR: 0.0041943, Loss: 20.5522,Val MAE: 358.0986\n",
            "Epoch: 249, LR: 0.0033554, Loss: 20.1862,Val MAE: 25.9182\n",
            "Epoch: 299, LR: 0.0017180, Loss: 21.7859,Val MAE: 10.9584\n",
            "Epoch: 349, LR: 0.0010995, Loss: 21.3241,Val MAE: 8.5966\n",
            "Epoch: 399, LR: 0.0007037, Loss: 22.5786,Val MAE: 17.7308\n",
            "Epoch: 449, LR: 0.0005629, Loss: 20.0262,Val MAE: 15.2174\n",
            "Epoch: 499, LR: 0.0003603, Loss: 20.9693,Val MAE: 7.9936\n",
            "Epoch: 549, LR: 0.0002306, Loss: 19.4646,Val MAE: 11.3357\n",
            "Epoch: 599, LR: 0.0001845, Loss: 22.0336,Val MAE: 8.4630\n",
            "Epoch: 649, LR: 0.0001181, Loss: 19.8118,Val MAE: 8.5338\n",
            "Epoch: 699, LR: 0.0000756, Loss: 20.8628,Val MAE: 7.6221\n",
            "Epoch: 749, LR: 0.0000387, Loss: 20.1285,Val MAE: 7.6349\n",
            "Epoch: 799, LR: 0.0000248, Loss: 17.1195,Val MAE: 7.5699\n",
            "Epoch: 849, LR: 0.0000158, Loss: 18.2042,Val MAE: 7.6960\n",
            "Epoch: 899, LR: 0.0000081, Loss: 20.1685,Val MAE: 8.3038\n",
            "Epoch: 949, LR: 0.0000052, Loss: 20.5965,Val MAE: 7.9228\n",
            "Epoch: 999, LR: 0.0000027, Loss: 23.7069,Val MAE: 7.7736\n",
            "Finish run with best_test=7.2882\n",
            "Epoch: 049, LR: 0.0160000, Loss: 29.4943,Val MAE: 391.8505\n",
            "Epoch: 099, LR: 0.0102400, Loss: 24.2980,Val MAE: 43.0237\n",
            "Epoch: 149, LR: 0.0052429, Loss: 23.9011,Val MAE: 59.8423\n",
            "Epoch: 199, LR: 0.0041943, Loss: 19.2759,Val MAE: 148.1403\n",
            "Epoch: 249, LR: 0.0021475, Loss: 21.1703,Val MAE: 38.5250\n",
            "Epoch: 299, LR: 0.0017180, Loss: 18.7942,Val MAE: 26.9965\n",
            "Epoch: 349, LR: 0.0010995, Loss: 20.7289,Val MAE: 13.0098\n",
            "Epoch: 399, LR: 0.0007037, Loss: 19.4617,Val MAE: 36.9466\n",
            "Epoch: 449, LR: 0.0003603, Loss: 18.1640,Val MAE: 21.4261\n",
            "Epoch: 499, LR: 0.0002306, Loss: 21.4708,Val MAE: 11.2942\n",
            "Epoch: 549, LR: 0.0001476, Loss: 22.2573,Val MAE: 9.1949\n",
            "Epoch: 599, LR: 0.0000944, Loss: 21.1159,Val MAE: 14.6425\n",
            "Epoch: 649, LR: 0.0000604, Loss: 21.4340,Val MAE: 9.0386\n",
            "Epoch: 699, LR: 0.0000387, Loss: 21.0984,Val MAE: 10.6344\n",
            "Epoch: 749, LR: 0.0000248, Loss: 20.3845,Val MAE: 8.0097\n",
            "Epoch: 799, LR: 0.0000127, Loss: 18.7085,Val MAE: 8.7195\n",
            "Epoch: 849, LR: 0.0000081, Loss: 20.9112,Val MAE: 8.9701\n",
            "Epoch: 899, LR: 0.0000042, Loss: 20.5539,Val MAE: 10.0782\n",
            "Epoch: 949, LR: 0.0000027, Loss: 18.9892,Val MAE: 9.8426\n",
            "Epoch: 999, LR: 0.0000017, Loss: 18.5224,Val MAE: 11.0823\n",
            "Finish run with best_test=8.3359\n",
            "Epoch: 049, LR: 0.0160000, Loss: 24.6544,Val MAE: 29.3580\n",
            "Epoch: 099, LR: 0.0102400, Loss: 23.6109,Val MAE: 113.2897\n",
            "Epoch: 149, LR: 0.0052429, Loss: 23.8925,Val MAE: 25.5134\n",
            "Epoch: 199, LR: 0.0041943, Loss: 21.6049,Val MAE: 219.4121\n",
            "Epoch: 249, LR: 0.0021475, Loss: 23.3898,Val MAE: 27.7693\n",
            "Epoch: 299, LR: 0.0013744, Loss: 23.9464,Val MAE: 16.7605\n",
            "Epoch: 349, LR: 0.0008796, Loss: 20.7848,Val MAE: 15.1601\n",
            "Epoch: 399, LR: 0.0005629, Loss: 22.4577,Val MAE: 12.5755\n",
            "Epoch: 449, LR: 0.0004504, Loss: 20.4754,Val MAE: 41.5318\n",
            "Epoch: 499, LR: 0.0002882, Loss: 23.6880,Val MAE: 15.8956\n",
            "Epoch: 549, LR: 0.0001845, Loss: 21.4350,Val MAE: 14.2175\n",
            "Epoch: 599, LR: 0.0001476, Loss: 20.5424,Val MAE: 12.4318\n",
            "Epoch: 649, LR: 0.0000756, Loss: 21.6257,Val MAE: 9.4206\n",
            "Epoch: 699, LR: 0.0000604, Loss: 22.5659,Val MAE: 8.8422\n",
            "Epoch: 749, LR: 0.0000387, Loss: 20.6543,Val MAE: 8.4917\n",
            "Epoch: 799, LR: 0.0000248, Loss: 19.2167,Val MAE: 8.3961\n",
            "Epoch: 849, LR: 0.0000158, Loss: 18.8029,Val MAE: 7.8956\n",
            "Epoch: 899, LR: 0.0000101, Loss: 18.6262,Val MAE: 7.8936\n",
            "Epoch: 949, LR: 0.0000052, Loss: 20.3466,Val MAE: 10.2524\n",
            "Epoch: 999, LR: 0.0000033, Loss: 19.7617,Val MAE: 9.7339\n",
            "Finish run with best_test=7.6633\n",
            "Epoch: 049, LR: 0.0200000, Loss: 25.7447,Val MAE: 525.5580\n",
            "Epoch: 099, LR: 0.0102400, Loss: 24.8949,Val MAE: 221.3724\n",
            "Epoch: 149, LR: 0.0065536, Loss: 25.8068,Val MAE: 37.9571\n",
            "Epoch: 199, LR: 0.0041943, Loss: 22.0576,Val MAE: 156.1107\n",
            "Epoch: 249, LR: 0.0026844, Loss: 21.8588,Val MAE: 45.7174\n",
            "Epoch: 299, LR: 0.0021475, Loss: 20.1567,Val MAE: 20.5091\n",
            "Epoch: 349, LR: 0.0010995, Loss: 19.1955,Val MAE: 70.2080\n",
            "Epoch: 399, LR: 0.0007037, Loss: 21.3907,Val MAE: 59.9442\n",
            "Epoch: 449, LR: 0.0004504, Loss: 17.3869,Val MAE: 37.0140\n",
            "Epoch: 499, LR: 0.0002882, Loss: 20.8976,Val MAE: 8.1361\n",
            "Epoch: 549, LR: 0.0001845, Loss: 18.5661,Val MAE: 14.0439\n",
            "Epoch: 599, LR: 0.0001476, Loss: 20.6311,Val MAE: 10.9776\n",
            "Epoch: 649, LR: 0.0000944, Loss: 20.9885,Val MAE: 9.8089\n",
            "Epoch: 699, LR: 0.0000604, Loss: 20.8835,Val MAE: 12.3921\n",
            "Epoch: 749, LR: 0.0000309, Loss: 18.9783,Val MAE: 8.1343\n",
            "Epoch: 799, LR: 0.0000198, Loss: 20.7526,Val MAE: 9.4837\n",
            "Epoch: 849, LR: 0.0000127, Loss: 19.0112,Val MAE: 8.5662\n",
            "Epoch: 899, LR: 0.0000065, Loss: 19.1854,Val MAE: 9.2065\n",
            "Epoch: 949, LR: 0.0000042, Loss: 18.9403,Val MAE: 8.7671\n",
            "Epoch: 999, LR: 0.0000027, Loss: 21.5751,Val MAE: 8.5745\n",
            "Finish run with best_test=8.0102\n",
            "Epoch: 049, LR: 0.0160000, Loss: 26.7092,Val MAE: 229.8397\n",
            "Epoch: 099, LR: 0.0102400, Loss: 25.1248,Val MAE: 228.4760\n",
            "Epoch: 149, LR: 0.0065536, Loss: 23.7576,Val MAE: 56.1902\n",
            "Epoch: 199, LR: 0.0052429, Loss: 23.9283,Val MAE: 203.7226\n",
            "Epoch: 249, LR: 0.0041943, Loss: 22.9287,Val MAE: 26.2493\n",
            "Epoch: 299, LR: 0.0026844, Loss: 21.7868,Val MAE: 25.1955\n",
            "Epoch: 349, LR: 0.0017180, Loss: 25.5476,Val MAE: 55.7081\n",
            "Epoch: 399, LR: 0.0008796, Loss: 19.6026,Val MAE: 43.9350\n",
            "Epoch: 449, LR: 0.0005629, Loss: 20.5850,Val MAE: 24.6706\n",
            "Epoch: 499, LR: 0.0003603, Loss: 18.7266,Val MAE: 14.2471\n",
            "Epoch: 549, LR: 0.0002306, Loss: 18.7418,Val MAE: 11.6581\n",
            "Epoch: 599, LR: 0.0001181, Loss: 20.7327,Val MAE: 10.2021\n",
            "Epoch: 649, LR: 0.0000756, Loss: 19.6193,Val MAE: 7.9938\n",
            "Epoch: 699, LR: 0.0000484, Loss: 20.1820,Val MAE: 8.2002\n",
            "Epoch: 749, LR: 0.0000309, Loss: 20.5103,Val MAE: 9.2227\n",
            "Epoch: 799, LR: 0.0000198, Loss: 23.4677,Val MAE: 8.4857\n",
            "Epoch: 849, LR: 0.0000101, Loss: 19.2356,Val MAE: 8.1831\n",
            "Epoch: 899, LR: 0.0000065, Loss: 20.5529,Val MAE: 10.9791\n",
            "Epoch: 949, LR: 0.0000033, Loss: 18.2937,Val MAE: 9.1535\n",
            "Epoch: 999, LR: 0.0000021, Loss: 23.2672,Val MAE: 10.5691\n",
            "Finish run with best_test=8.4040\n",
            "Epoch: 049, LR: 0.0160000, Loss: 23.5275,Val MAE: 64.9291\n",
            "Epoch: 099, LR: 0.0081920, Loss: 22.1170,Val MAE: 52.9741\n",
            "Epoch: 149, LR: 0.0052429, Loss: 24.0353,Val MAE: 213.3374\n",
            "Epoch: 199, LR: 0.0033554, Loss: 20.9986,Val MAE: 144.1674\n",
            "Epoch: 249, LR: 0.0021475, Loss: 21.0698,Val MAE: 53.4753\n",
            "Epoch: 299, LR: 0.0017180, Loss: 23.9540,Val MAE: 50.4213\n",
            "Epoch: 349, LR: 0.0008796, Loss: 20.8245,Val MAE: 54.6267\n",
            "Epoch: 399, LR: 0.0005629, Loss: 19.0802,Val MAE: 14.8718\n",
            "Epoch: 449, LR: 0.0004504, Loss: 19.1120,Val MAE: 9.5965\n",
            "Epoch: 499, LR: 0.0002306, Loss: 23.1178,Val MAE: 16.5986\n",
            "Epoch: 549, LR: 0.0001845, Loss: 23.1910,Val MAE: 9.3729\n",
            "Epoch: 599, LR: 0.0001181, Loss: 19.5693,Val MAE: 10.5568\n",
            "Epoch: 649, LR: 0.0000944, Loss: 19.8838,Val MAE: 11.7021\n",
            "Epoch: 699, LR: 0.0000604, Loss: 21.4212,Val MAE: 8.4302\n",
            "Epoch: 749, LR: 0.0000387, Loss: 20.7160,Val MAE: 8.8485\n",
            "Epoch: 799, LR: 0.0000248, Loss: 21.9953,Val MAE: 9.3285\n",
            "Epoch: 849, LR: 0.0000158, Loss: 18.1973,Val MAE: 9.2968\n",
            "Epoch: 899, LR: 0.0000101, Loss: 20.2423,Val MAE: 8.4723\n",
            "Epoch: 949, LR: 0.0000065, Loss: 18.0765,Val MAE: 9.8106\n",
            "Epoch: 999, LR: 0.0000033, Loss: 22.9114,Val MAE: 10.6755\n",
            "Finish run with best_test=8.0234\n",
            "Epoch: 049, LR: 0.0160000, Loss: 27.2868,Val MAE: 65.9466\n",
            "Epoch: 099, LR: 0.0128000, Loss: 22.1839,Val MAE: 70.9270\n",
            "Epoch: 149, LR: 0.0081920, Loss: 23.7469,Val MAE: 178.1213\n",
            "Epoch: 199, LR: 0.0041943, Loss: 23.8552,Val MAE: 36.3673\n",
            "Epoch: 249, LR: 0.0026844, Loss: 23.6486,Val MAE: 226.5005\n",
            "Epoch: 299, LR: 0.0017180, Loss: 22.1591,Val MAE: 218.6330\n",
            "Epoch: 349, LR: 0.0010995, Loss: 22.1022,Val MAE: 86.1621\n",
            "Epoch: 399, LR: 0.0007037, Loss: 20.0261,Val MAE: 75.0557\n",
            "Epoch: 449, LR: 0.0004504, Loss: 20.8576,Val MAE: 61.9756\n",
            "Epoch: 499, LR: 0.0002882, Loss: 21.8013,Val MAE: 8.0233\n",
            "Epoch: 549, LR: 0.0001476, Loss: 21.9324,Val MAE: 8.5793\n",
            "Epoch: 599, LR: 0.0001181, Loss: 20.7445,Val MAE: 11.8711\n",
            "Epoch: 649, LR: 0.0000756, Loss: 20.5380,Val MAE: 11.3948\n",
            "Epoch: 699, LR: 0.0000484, Loss: 19.7091,Val MAE: 12.5019\n",
            "Epoch: 749, LR: 0.0000309, Loss: 22.4056,Val MAE: 8.4456\n",
            "Epoch: 799, LR: 0.0000198, Loss: 19.9665,Val MAE: 8.2245\n",
            "Epoch: 849, LR: 0.0000101, Loss: 18.8506,Val MAE: 8.5455\n",
            "Epoch: 899, LR: 0.0000065, Loss: 19.5188,Val MAE: 9.2248\n",
            "Epoch: 949, LR: 0.0000042, Loss: 19.6120,Val MAE: 8.0737\n",
            "Epoch: 999, LR: 0.0000021, Loss: 20.7629,Val MAE: 9.6422\n",
            "Finish run with best_test=8.2469\n",
            "Epoch: 049, LR: 0.0160000, Loss: 24.7015,Val MAE: 52.4294\n",
            "Epoch: 099, LR: 0.0102400, Loss: 22.7544,Val MAE: 409.8875\n",
            "Epoch: 149, LR: 0.0052429, Loss: 21.4336,Val MAE: 442.2713\n",
            "Epoch: 199, LR: 0.0041943, Loss: 21.0740,Val MAE: 34.8606\n",
            "Epoch: 249, LR: 0.0033554, Loss: 23.1884,Val MAE: 339.8577\n",
            "Epoch: 299, LR: 0.0021475, Loss: 21.8716,Val MAE: 100.8062\n",
            "Epoch: 349, LR: 0.0013744, Loss: 19.2648,Val MAE: 101.9558\n",
            "Epoch: 399, LR: 0.0010995, Loss: 22.4099,Val MAE: 19.2439\n",
            "Epoch: 449, LR: 0.0007037, Loss: 19.4062,Val MAE: 32.5006\n",
            "Epoch: 499, LR: 0.0004504, Loss: 21.2516,Val MAE: 51.0560\n",
            "Epoch: 549, LR: 0.0002882, Loss: 18.9938,Val MAE: 26.3421\n",
            "Epoch: 599, LR: 0.0001845, Loss: 21.0655,Val MAE: 8.1373\n",
            "Epoch: 649, LR: 0.0001181, Loss: 19.3829,Val MAE: 17.2800\n",
            "Epoch: 699, LR: 0.0000756, Loss: 22.0205,Val MAE: 11.1532\n",
            "Epoch: 749, LR: 0.0000484, Loss: 17.8427,Val MAE: 7.7803\n",
            "Epoch: 799, LR: 0.0000309, Loss: 20.7249,Val MAE: 8.6541\n",
            "Epoch: 849, LR: 0.0000198, Loss: 20.3998,Val MAE: 8.8946\n",
            "Epoch: 899, LR: 0.0000101, Loss: 19.3046,Val MAE: 7.9438\n",
            "Epoch: 949, LR: 0.0000065, Loss: 19.7208,Val MAE: 8.7442\n",
            "Epoch: 999, LR: 0.0000042, Loss: 19.6888,Val MAE: 8.6330\n",
            "Finish run with best_test=7.5073\n",
            "Epoch: 049, LR: 0.0160000, Loss: 22.7603,Val MAE: 170.9051\n",
            "Epoch: 099, LR: 0.0102400, Loss: 26.9468,Val MAE: 51.9357\n",
            "Epoch: 149, LR: 0.0052429, Loss: 24.9709,Val MAE: 28.3807\n",
            "Epoch: 199, LR: 0.0033554, Loss: 20.1014,Val MAE: 82.9796\n",
            "Epoch: 249, LR: 0.0021475, Loss: 20.4656,Val MAE: 67.5755\n",
            "Epoch: 299, LR: 0.0010995, Loss: 21.5549,Val MAE: 42.8000\n",
            "Epoch: 349, LR: 0.0008796, Loss: 18.4960,Val MAE: 62.4161\n",
            "Epoch: 399, LR: 0.0004504, Loss: 20.7342,Val MAE: 22.6735\n",
            "Epoch: 449, LR: 0.0003603, Loss: 18.1396,Val MAE: 11.5912\n",
            "Epoch: 499, LR: 0.0002882, Loss: 22.5666,Val MAE: 12.0026\n",
            "Epoch: 549, LR: 0.0001845, Loss: 17.7173,Val MAE: 11.2150\n",
            "Epoch: 599, LR: 0.0001181, Loss: 19.8248,Val MAE: 8.5195\n",
            "Epoch: 649, LR: 0.0000756, Loss: 20.4631,Val MAE: 8.6896\n",
            "Epoch: 699, LR: 0.0000604, Loss: 22.1838,Val MAE: 8.8140\n",
            "Epoch: 749, LR: 0.0000387, Loss: 21.1110,Val MAE: 9.4546\n",
            "Epoch: 799, LR: 0.0000248, Loss: 21.2484,Val MAE: 8.0326\n",
            "Epoch: 849, LR: 0.0000158, Loss: 19.0660,Val MAE: 8.9935\n",
            "Epoch: 899, LR: 0.0000101, Loss: 18.5367,Val MAE: 11.5213\n",
            "Epoch: 949, LR: 0.0000052, Loss: 19.4574,Val MAE: 8.2402\n",
            "Epoch: 999, LR: 0.0000033, Loss: 21.4723,Val MAE: 8.4786\n",
            "Finish run with best_test=8.8316\n",
            "Epoch: 049, LR: 0.0160000, Loss: 28.4441,Val MAE: 169.2502\n",
            "Epoch: 099, LR: 0.0128000, Loss: 24.2751,Val MAE: 186.6395\n",
            "Epoch: 149, LR: 0.0081920, Loss: 23.1648,Val MAE: 54.9321\n",
            "Epoch: 199, LR: 0.0041943, Loss: 20.1205,Val MAE: 247.0035\n",
            "Epoch: 249, LR: 0.0026844, Loss: 21.0559,Val MAE: 150.3351\n",
            "Epoch: 299, LR: 0.0017180, Loss: 20.1709,Val MAE: 103.7380\n",
            "Epoch: 349, LR: 0.0013744, Loss: 23.1459,Val MAE: 31.9937\n",
            "Epoch: 399, LR: 0.0008796, Loss: 19.5074,Val MAE: 141.1469\n",
            "Epoch: 449, LR: 0.0007037, Loss: 20.8500,Val MAE: 32.0893\n",
            "Epoch: 499, LR: 0.0003603, Loss: 20.8510,Val MAE: 28.2283\n",
            "Epoch: 549, LR: 0.0002306, Loss: 21.9684,Val MAE: 9.0880\n",
            "Epoch: 599, LR: 0.0001476, Loss: 19.3405,Val MAE: 9.1177\n",
            "Epoch: 649, LR: 0.0000756, Loss: 19.5987,Val MAE: 9.4349\n",
            "Epoch: 699, LR: 0.0000484, Loss: 19.4547,Val MAE: 7.8741\n",
            "Epoch: 749, LR: 0.0000309, Loss: 20.6584,Val MAE: 9.0165\n",
            "Epoch: 799, LR: 0.0000198, Loss: 19.2707,Val MAE: 8.3912\n",
            "Epoch: 849, LR: 0.0000101, Loss: 19.4651,Val MAE: 7.7525\n",
            "Epoch: 899, LR: 0.0000065, Loss: 19.9034,Val MAE: 7.9761\n",
            "Epoch: 949, LR: 0.0000042, Loss: 19.3851,Val MAE: 9.3499\n",
            "Epoch: 999, LR: 0.0000033, Loss: 19.4009,Val MAE: 8.5720\n",
            "Finish run with best_test=7.3517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparison with related work \n",
        "- (A slight different/easier dataset QM7, same Colomb Matrix (CM) representation):\n",
        " -  [Rupp et al.](https://arxiv.org/pdf/1109.2618.pdf) achieved Atomization Energies MAE ~10 (target_variable=0), using eigenvalues of CM; \n",
        " - [Montavon et al.](https://proceedings.neurips.cc/paper/2012/file/115f89503138416a242f40fb7d7f338e-Paper.pdf) achieve Atomization Energies MAE < 4 using random permutations (of rows and columns) of CM. It suggests that building an (approximate) equivariant model with data augmentation yields better performance (than exact invariant model, i.e., using eigenvalues)\n",
        "- QM7b:\n",
        " - [Motavon et al.[https://th.fhi-berlin.mpg.de/site/uploads/Publications/QM-NJP_20130315.pdf]: Table 1 (really good results but require high memory storage, by first converting CM to binary representation (2000 x 23 x 23) Tensor\n",
        " - [Wu et al.](https://arxiv.org/pdf/1703.00564.pdf): Table 10 (similar to ours but use Kernel-ridge-regression for CM)\n",
        "\n",
        "More refs: http://quantum-machine.org/datasets/\n",
        "Better representation than CM: https://aip.scitation.org/doi/10.1063/1.5126701\n",
        "(out of the scope?) (maybe can ask Erik if we go down the rabbit hole of this?)"
      ],
      "metadata": {
        "id": "iwdYkfjxsYra"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dF-FLdvGIJp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Invariant classification) Benchmark dataset: ModelNet40\n",
        "- The most popular dataset for pointcloud classification\n",
        "- Can benchmark against Deepset and PointNet (old SOTA models): e,g, https://github.com/manzilzaheer/DeepSets/blob/master/PointClouds/run.py\n",
        "- BUG: relatively large point clouds, our methods need $O(n^2)$ computation whereas Deepset/PointNet can do $O(n)$ (since the point clouds literally live in $d=3$)"
      ],
      "metadata": {
        "id": "iFdxli69QGTP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfxM83KR7bCp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}